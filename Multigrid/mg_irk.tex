\documentclass[a4paper,10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb,amsfonts,stmaryrd}
\usepackage{mathtools}
\usepackage{soul}
\usepackage{array}
\usepackage{empheq}
\usepackage{xfrac}
\usepackage{minibox}
\usepackage{enumitem}
	\setlist{nosep} % or \setlist{noitemsep} to leave space around whole list
\usepackage{color}
\usepackage{blkarray}
\setcounter{MaxMatrixCols}{20}
\usepackage{showlabels}
\usepackage{arydshln}	% Dotted lines in arrays
\usepackage{adjustbox}
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}
\usepackage{cleveref}


\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\crefname{assumption}{Assumption}{Assumptions}

\newcommand{\tcb}{\textcolor{blue}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO\@: #1]}}


\begin{document}
\allowdisplaybreaks

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\section{Multigrid for fully implicit Runge-Kutta}\label{sec:mg}

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{The problem}\label{sec:mg:prob}

Here we explore a semi-algebraic (block) multigrid solver for the block
linear systems that arise in fully implicit Runge-Kutta and discontinuous
Galerkin in time.
Consider the method-of-lines approach to the numerical solution of partial differential
equations (PDEs), where we discretize in space and arrive at a system of ordinary
differential equations (ODEs) in time,
%
\begin{align}\label{eq:problem}
	M\mathbf{u}'(t) =  \mathcal{N}(\mathbf{u},t) \quad\text{in }(0,T], \quad \mathbf{u}(0) = \mathbf{u}_0,
\end{align}
%
where $M$ is a mass matrix and $\mathcal{N}\in\mathbb{R}^{N\times N}$ is a discrete,
time-dependent, nonlinear operator depending on $t$ and $\mathbf{u}$ (including potential
forcing terms). Note, PDEs with an algebraic constraint, for example, the divergence-free
constraint in Navier Stokes, instead yield a system of differential algebraic equations
(DAEs), which are not yet addressed. 
Now, consider time propagation of \eqref{eq:problem} using an $s$-stage
Runge-Kutta scheme, characterized by the Butcher tableaux 
%
\begin{align*}
	\renewcommand\arraystretch{1.2}
	\begin{array}
	{c|c}
	\mathbf{c}_0 & A_0\\
	\hline
	& \mathbf{b}_0^T
	\end{array},
\end{align*}
%
with Runge-Kutta matrix $A_0 = (a_{ij})$, weight vector $\mathbf{b}_0^T = (b_1, \ldots, b_s)^T$,
and quadrature nodes $\mathbf{c}_0 = (c_0, \ldots, c_s)$.

Runge-Kutta methods update the solution using a sum over stage vectors,
%
\begin{align}\label{eq:update}
\mathbf{u}_{n+1} & = \mathbf{u}_n + \delta t \sum_{i=1}^s b_i\mathbf{k}_i, 
	\hspace{5ex}\textnormal{where}\\
M\mathbf{k}_i & = \mathcal{N}\left(\mathbf{u}_n + \delta t\sum_{j=1}^s a_{ij}\mathbf{k}_j, t_n+\delta tc_i\right).\label{eq:stages}
\end{align}
%
For nonlinear PDEs, $\mathcal{N}$ is linearized using, for example, a Newton or a
Picard linearization, and each nonlinear iteration then consists of solving the
linearized system of equations. In most cases, such a linearization
is designed to approximate (or equal) the Jacobian of \eqref{eq:stages}. Applying
a chain rule to \eqref{eq:stages} for the partial
$\partial(M\mathbf{k}_i-\mathcal{N}_i)/\partial\mathbf{k}_j$, we see that
the linearized system should take the form
%
\begin{align}\label{eq:k0}
\left( \begin{bmatrix} M  & & \mathbf{0} \\ & \ddots \\ \mathbf{0} & & M\end{bmatrix}
	- \delta t \begin{bmatrix} a_{11}\mathcal{L}_1 & ... & a_{1s}\mathcal{L}_1 \\
	\vdots & \ddots & \vdots \\ a_{s1}\mathcal{L}_s & ... & a_{ss} \mathcal{L}_s \end{bmatrix} \right)
	\begin{bmatrix} \mathbf{k}_1 \\ \vdots \\ \mathbf{k}_s \end{bmatrix} 
& = \begin{bmatrix} \mathbf{f}_1 \\ \vdots \\ \mathbf{f}_s \end{bmatrix},
\end{align}
%
where $\mathcal{L}_i\in\mathbb{R}^{N\times N}$ denotes a linearization of
the nonlinear function corresponding to the $i$th stage vector,
$\mathcal{N}_i\coloneqq \mathcal{N}\left(\mathbf{u}_n + \delta t\sum_{j=1}^s
a_{ij}\mathbf{k}_j, t_n+\delta tc_i\right)$ (the main point being that the
spatially linearized operators, $\mathcal{L}_i$, should be fixed for a
given block row of the full linearized system, as in \eqref{eq:k0}).

Define the field of values of operator $\mathcal{L}$ as the set
%
\begin{align}\label{eq:fov}
W(\mathcal{L}) \coloneqq \left\{ \langle \mathcal{L}\mathbf{x},\mathbf{x}\rangle \text{ : }
	\|\mathbf{x}\| = 1 \right\}.
\end{align}
Here, we make two reasonable assumptions on $A_0$ and $\mathcal{L}_i$:
%
\begin{assumption}\label{ass:eig}
Assume that all eigenvalues of $A_0$ (and equivalently $A_0^{-1})$ have positive real part.
\end{assumption}
\begin{assumption}\label{ass:fov}
Let $\mathcal{L}$ be a linearized spatial operator, and assume that $W(\mathcal{L}) \leq 0$.
\end{assumption}
%
\noindent
In addition, it is worth pointing out that $A_0$ typically has a dominant
lower triangular structure.

For ease of notation, let us scale on the left by a block-diagonal matrix
with diagonal blocks $M^{-1}$ and redefine $\mathcal{L}_i \mapsto
\delta tM^{-1}\mathcal{L}_i$. The scaled system then takes the form
%
\begin{align}\label{eq:k1}
\left( \begin{bmatrix} M & & \mathbf{0} \\ & \ddots \\ \mathbf{0} & & M \end{bmatrix}
	- \begin{bmatrix} a_{11}\mathcal{L}_1 & ... & a_{1s}\mathcal{L}_1 \\
	\vdots & \ddots & \vdots \\ a_{s1}\mathcal{L}_s & ... & a_{ss} \mathcal{L}_s \end{bmatrix} \right)
	\begin{bmatrix} \mathbf{k}_1 \\ \vdots \\ \mathbf{k}_s \end{bmatrix} 
& = \begin{bmatrix} \mathbf{f}_1 \\ \vdots \\ \mathbf{f}_s \end{bmatrix}.
\end{align}
%
Unfortunately, we cannot say anything about the field-of-values of the latter operator.
This is easily verified by noting that the eigenvalues of $A_0\otimes \mathcal{L}$
are given by the product of eigenvalues of $A_0$ and $\mathcal{L}$, and one can easily
construct $A_0$ and $\mathcal{L}$ that satisfy \Cref{ass:eig,ass:fov} with eigenvalues
in both half planes. This also confuses what the ``small''/``smooth'' modes of
\eqref{eq:k1} are. 

For this reason, we also consider a modified set of equations obtained by pulling
out an $A_0\otimes I$ from \eqref{eq:k1}. Let $\{\alpha_{ij}\}$ denote the entries
of $A_0^{-1}$, and consider an equivalent formulation of \eqref{eq:k1},

%
\begin{align}\label{eq:k2}
\left( \begin{bmatrix} \alpha_{11} M & ... & \alpha_{1s}M  \\ \vdots & \ddots & \vdots
		\\ \alpha_{s1}M &  ... & \alpha_{ss}M\end{bmatrix}
	- \begin{bmatrix} \mathcal{L}_1 &  \\
	 & \ddots & \\ & & \mathcal{L}_s \end{bmatrix} \right) 
	(A_0\otimes I)
	\begin{bmatrix} \mathbf{k}_1 \\ \vdots \\ \mathbf{k}_s \end{bmatrix} 
& = \begin{bmatrix} \mathbf{f}_1 \\ \vdots \\ \mathbf{f}_s \end{bmatrix}.
\end{align}
%
We consider the system matrix we have to solve in \eqref{eq:k2} as the leading
block matrix. Inverting this solves for the scaled stage vectors $(A_0\otimes I)\mathbf{K}$,
and we can then easily invert $A_0\otimes I$ to get the real stage vectors.
Here we have nicer field-of-value measures. In particular, note that the first
operator $A_0^{-1}\otimes I$ satisfies $W(A_0^{-1}\otimes I) \geq 0$ and the second operator
satisfies $W(\textnormal{diag}(\mathcal{L}_i)) \leq 0$. To that end, if we consider
``smooth'' modes to be the smallest modes with respect to eigenvalues or FOVs,
the ``smooth'' modes of \eqref{eq:k2} are largely defined by the ``smooth'' modes
of diag$(\mathcal{L}_i)$. If $\mathcal{L}_i = \mathcal{L}_j$ for all $i,j$, it
stands to reason that the block constant vector is a good representation of the
null space. For $\mathcal{L}_i \neq \mathcal{L}_j$, we still expect that the
operators are similar in some sense, so the block constant vector is still
likely to be a reasonable approximation to the near null space.

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{Current solvers}\label{sec:mg:solver}





% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{Multigrid}\label{sec:mg:mg}


% ------------------------------------------------------------------------------------- %
\subsubsection{Using singular vectors}\label{sec:mg:mg:sv}

Although the eigenvalues of $A_0^{-1}$ are relatively $\mathcal{O}(1)$ in magnitude,
the singular values have more significant variation, e.g., varying from $1-40$ for
Gauss10. In addition to choosing a coarse grid operator that represents the ``smooth''
modes of $\{\mathcal{L}_i\}$, we also may want to account for the small modes of
$A_0^{-1}$. Thus let $A_0^{-1} := U\Sigma V^*$ and let $\mathbf{v}^s$ and
$\mathbf{u}^s$ denote the $s$th right and left singular vector (corresponding
to the smallest singular value). Then, define 
%
\begin{align*} 
R = \begin{bmatrix} {u}^s_1 I & \hdots & {u}^s_s I \end{bmatrix},
\hspace{5ex}
P^T = \begin{bmatrix} {v}^s_1 I & \hdots & {v}^s_s I \end{bmatrix}.
\end{align*}
%
This yields a coarse-grid operator
%
\begin{align}\nonumber
RAP & \coloneqq \begin{bmatrix} {u}^s_1 I & \hdots & {u}^s_s I \end{bmatrix}
	\begin{bmatrix} \alpha_{11} M - \mathcal{L}_1 & ... & \alpha_{1s}M  \\
		\vdots & \ddots & \vdots \\ \alpha_{s1}M &  ... & \alpha_{ss}M - \mathcal{L}_s\end{bmatrix}
	\begin{bmatrix} {v}^s_1 I \\ \vdots \\ {v}^s_s I\end{bmatrix} \\
& = \sigma_sM - \sum_{i=1}^s {u}^s_i{v}^s_i\mathcal{L}_i \label{eq:cg_sv},
\end{align}
%
and restricted operator
%
\begin{align}\nonumber
RA & = \begin{bmatrix} {u}^s_1 I & \hdots & {u}^s_s I \end{bmatrix}
	\begin{bmatrix} \alpha_{11} M - \mathcal{L}_1 & ... & \alpha_{1s}M  \\
		\vdots & \ddots & \vdots \\ \alpha_{s1}M &  ... & \alpha_{ss}M - \mathcal{L}_s\end{bmatrix} \\
& = \begin{bmatrix} \sigma_s{v}^s_1M - {u}^s_1 \mathcal{L}_1 & \hdots &
	\sigma_s{v}^s_sM - {u}^s_s \mathcal{L}_s \end{bmatrix}\label{eq:ra_sv}
\end{align}
%

Now consider the simplification $\mathcal{L}_i = \mathcal{L}_j$ and let $c :=
(\mathbf{u}^s\cdot\mathbf{v}^s)$. Then, coarse-grid correction takes the form
%
\begin{align*}
& \hspace{-2ex} I - P(RAP)^{-1}RA = I - 
	\begin{bmatrix} {v}^s_1 I \\ \vdots \\ {v}^s_s I\end{bmatrix}
	(\sigma_sM - c\mathcal{L})^{-1}
	\begin{bmatrix} \sigma_s{v}^s_1M - {u}^s_1 \mathcal{L}_1 & \hdots &
		\sigma_s{v}^s_sM - {u}^s_s \mathcal{L}_s \end{bmatrix} \\
& = I -  \begin{bmatrix} {v}^s_1 I \\ \vdots \\ {v}^s_s I\end{bmatrix}
	(\sigma_sM - c\mathcal{L})^{-1}
	\begin{bmatrix} \tfrac{{u}^s_1}{c}(\sigma_sM - c\mathcal{L} +
		\sigma_s(\tfrac{{v}^s_1c}{{u}^s_1} - 1)M) & \hdots &
	\tfrac{{u}^s_s}{c}(\sigma_sM - c\mathcal{L} +
		\sigma_s(\tfrac{{v}^s_sc}{{u}^s_s} - 1)M) \end{bmatrix} \\
& = I - \frac{1}{\mathbf{u}^s\cdot\mathbf{v}^s}
	\begin{bmatrix} {v}^s_1 I \\ \vdots \\ {v}^s_s I\end{bmatrix}
	\begin{bmatrix} {u}^s_1 I & \hdots & {u}^s_s I\end{bmatrix}
	- \sigma_s\begin{bmatrix} {v}^s_1 I \\ \vdots \\ {v}^s_s I\end{bmatrix}
	(\sigma_sM - c\mathcal{L})^{-1}
	\begin{bmatrix} ({v}^s_1 - \tfrac{u^s_1}{\mathbf{u}^s\cdot\mathbf{v}^s}) M &
		\hdots & ({v}^s_s - \tfrac{u^s_s}{\mathbf{u}^s\cdot\mathbf{v}^s}) M\end{bmatrix} \\
& = I - \frac{1}{\mathbf{u}^s\cdot\mathbf{v}^s}
	\begin{bmatrix} {v}^s_1 I \\ \vdots \\ {v}^s_s I\end{bmatrix}
	\begin{bmatrix} {u}^s_1 I & \hdots & {u}^s_s I\end{bmatrix}
	- \begin{bmatrix} {v}^s_1 I \\ \vdots \\ {v}^s_s I\end{bmatrix}
	(M - \tfrac{c}{\sigma_s}\mathcal{L})^{-1}
	\begin{bmatrix} ({v}^s_1 - \tfrac{u^s_1}{\mathbf{u}^s\cdot\mathbf{v}^s}) M &
		\hdots & ({v}^s_s - \tfrac{u^s_s}{\mathbf{u}^s\cdot\mathbf{v}^s}) M \end{bmatrix}
\end{align*}
%

I cannot find information in the literature regarding Butcher tableaux and
their corresponding singular value decompositions. Here we note a few interesting
properties we have observed for Gauss and Radau integration schemes:
%
\begin{enumerate}
	\item $\sigma_s > 1$ and $\sigma_s\sim\mathcal{O}(1)$.
	
	\item $\mathbf{u}^s$ and $\mathbf{v}^s$ resemble standard geometrically
	``smooth'' modes, both roughly looking like a sine-hump. Moreover, they are
	strictly positive for Gauss and Radau (up to 6 stages), while all other left
	and right singular vectors are oscillatory.

	\item 

	% \sigma_{min}, u_s \cdot v_s, order
	% Gauss
	% 1.452225437697755, 0.5982591455680424, 12
	% 1.464865154102248, 0.5998594999185906, 10
	% 1.483366262535218, 0.6017267399644856, 8
	% 2.14746, 0.607471, 6
	% 1.58258, 0.654654, 4
	% Radau 
	% 1.14001, 0.624695, 3
	% 1.24637, 0.57189, 5
	% 1.283833915511943, 0.5700226765828628, 7
	% 1.305918094874295, 0.5723806804683383, 9
	% 1.320139561701621, 0.5745073671991398, 11



	\item
\end{enumerate}

\textbf{Complementary relaxation:}\tcb{(This was written for aggregation,
not SVD approach; update.)}
The next question is how to develop a complementary relaxation scheme. In
particular, we want to make the error over all stages roughly similar. One
heuristic to do so is by noting that $\{\mathcal{L}_i\}$ should be similar operators
for all $i$ and, thus, we particularly expect the ``smooth'' modes of these
operators to be similar for all $\mathcal{L}_i$. Thus consider a relaxation
scheme that eliminates high frequency error on each stage, which would
hypothetically leave similar low-frequency error on all stages. Due to 
the coupling in the larger system, I think we want something better than
just a straight Gauss-Seidel type relaxation on the full system. In
particular, we don't want to smooth error on the full system, we want
to smooth error on each stage. To that end, let $A_0 = Q_0R_0Q_0^T$
be the real Schur composition of $A_0$, where $Q_0Q_0^T = I$ and $R_0$
is block triangular, with $1\times 1$ blocks corresponding to real
eigenvalues of $A_0$ and $2\times 2$ blocks corresponding to complex
eigenvalues of $A_0$. Returning to \eqref{eq:k2} with $\mathcal{L}_i =
\mathcal{L}_j$, we have
%
\begin{align}\nonumber
A_0^{-1}\otimes I - I\otimes \mathcal{L}
	& = (Q_0\otimes I) \left( R_0\otimes I -I\otimes \mathcal{L}\right)(Q_0^T\otimes I), \\
\left( A_0^{-1}\otimes I -I\otimes \mathcal{L}\right)^{-1}
	& = (Q_0\otimes I) \left( R_0\otimes I - I\otimes \mathcal{L}\right)^{-1}(Q_0^T\otimes I).
	\label{eq:inv_kron}
\end{align}
%
Applying $Q_0\otimes I$ and $Q_0^T\otimes I$ are fairly trivial computations
and only require linear combinations of different stage vectors. Thus a
simple approximation to \eqref{eq:inv_kron} is to replace $\mathcal{L} \mapsto
M$, where $M$ is some easy to invert approximation of $\mathcal{L}$, such as
the lower-triangular part. This yields a relaxation scheme
%
\begin{align}\label{eq:relax1}
\mathcal{M}^{-1} & = (Q_0\otimes I) \left( R_0\otimes I - 
	I\otimes {M}\right)^{-1}(Q_0^T\otimes I).
\end{align}
%
A similar relaxation scheme can be developed for the more general setting, and
we simple replace $\mathcal{L}_i\mapsto M_i$
%
\begin{align}\label{eq:relax2}
\mathcal{M}^{-1} & = (Q_0\otimes I) \left( R_0\otimes I - 
	\begin{bmatrix} M_1 \\ & \ddots \\ && M_s\end{bmatrix} \right)^{-1}(Q_0^T\otimes I).
\end{align}
%
Note this version is less exact because $Q_0\otimes I$ does not commute with
diag$\{\mathcal{L}_i\}$, but it should accomplish a similar objective.

The one outstanding question of relaxation schemes in \eqref{eq:relax1} and
\eqref{eq:relax2} is inverting the block $2\times 2$ systems that arise
corresponding to complex eigenvalues of $A_0$, $\lambda = \eta \pm \mathrm{i}\beta$,
e.g.,
%
\begin{align}\label{eq:rel_sys}
\begin{bmatrix} \eta I - M_1 & -\phi I \\ -\frac{\beta^2}{\phi}I & \eta I - M_2 \end{bmatrix}
\end{align}
%
If we choose $M_i$ to be the lower triangular portion of $\mathcal{L}_i$, we
can reorder \eqref{eq:rel_sys} to be block lower triangular with $2\times 2$
blocks. This is not currently implemented, but probably would not be too hard.
Alternatively, we could just apply the theory in the nonlinear paper and use
a modified $\gamma_*$ relaxation constant in the $(2,2)$-block.









\end{document}

