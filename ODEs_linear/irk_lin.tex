% \documentclass[a4paper,10pt]{article}
\documentclass[review]{siamart}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{multirow}
\usepackage{empheq}
\usepackage{enumitem}
	\setlist{nosep} % or \setlist{noitemsep} to leave space around whole list
\usepackage{color}
\usepackage{verbatim}
% \usepackage{showlabels}
\usepackage{stmaryrd}
\usepackage{adjustbox}
\usepackage{hyperref}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0}
\usepackage[numbers,sort]{natbib}
\usepackage{cleveref}
\usepackage[belowskip=-5pt]{subcaption}
\usepackage{grffile}

\newsiamremark{remark}{Remark}
\newsiamremark{assumption}{Assumption}


% \newtheorem{lemma}{Lemma}
% \newtheorem{definition}{Definition}
% \newtheorem{theorem}{Theorem}
% \newtheorem{corollary}{Corollary}

\newcommand{\tcb}{\textcolor{blue}}
\newcommand{\tcp}{\textcolor{purple}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO\@: #1]}}

\newcommand{\WP}[1]{\textcolor{red}{WP: #1}}
\newcommand{\OK}[1]{\textcolor{blue}{[OK: #1]}}

\DeclareMathOperator{\adj}{adj}

% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %

\newcommand{\TheTitle}{Fast parallel solution of fully implicit Runge-Kutta and discontinuous
	Galerkin in time for numerical PDEs, Part I: the linear setting}
\newcommand{\TheAuthors}{B.S. Southworth, O.A. Krzysik, W. Pazner, and H. De Sterck}
\headers{Parallel solution of fully implict Runge-Kutta and DG in time I}{\TheAuthors}
\title{{\TheTitle}\thanks{This work was  performed under the auspices of the U.S.
Department of Energy under contract DE-AC52-07NA27344 and Lawrence Livermore National Laboratory under  contract B639443. OAK acknowledges the support of an Australian Government Research Training Program (RTP) Scholarship.}}

\author{
	Ben S. Southworth\thanks{Department of Applied Mathematics,
    University of Colorado,
    U.S.A. (\url{ben.s.southworth@gmail.com}),
    \url{http://orcid.org/0000-0002-0283-4928}}
\and
    Oliver A. Krzysik\thanks{School of Mathematics, Monash University,
  	Australia (\url{oliver.krzysik@monash.edu}),
  	\url{https://orcid.org/0000-0001-7880-6512}}
\and
  	Will Pazner\thanks{Center for Applied Scientific Computing, Lawrence Livermore National Laboratory,
    U.S.A. (\url{pazner1@llnl.gov})}
\and
    Hans De Sterck\thanks{Department of Applied Mathematics,
  	University of Waterloo,
  	Waterloo, Canada
  	(\url{hdesterck@uwaterloo.ca})}
}

\ifpdf%
\hypersetup{%
  pdftitle={\TheTitle},
  pdfauthor={\TheAuthors}
}
\fi

% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %

\begin{document}
\maketitle
\allowdisplaybreaks

\begin{abstract}
This paper introduces a theoretical and algorithmic framework for the fast, parallel
solution of fully implicit Runge-Kutta methods in linear numerical PDEs (without algebraic
constraints), which also naturally applies to discontinuous Galerkin discretizations
in time. The new
method can be used with arbitrary existing preconditioners for backward-Euler type
time stepping schemes, and is amenable to the use of three-term recursion Krylov
methods when the underlying spatial discretization is.
Under quite general assumptions on the spatial discretization that yield stable time
integration, the preconditioned operator is proven to have conditioning $\sim\mathcal{O}(1)$,
with only weak dependence on number of stages/polynomial order. The new method is
demonstrated to be effective on various high-order finite-difference and
finite-element discretizations of linear parabolic and hyperbolic problems,
demonstrating fast, scalable solution of up to 10th order accuracy. In several
cases, the new method can achieve 4th-order accuracy using Gauss integration
with roughly half the number of preconditioner applications as required using
standard SDIRK techniques.
\end{abstract}


% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\section{Introduction}\label{sec:intro}

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{Fully implicit Runge-Kutta}\label{sec:intro:irk}

Consider the method-of-lines approach to the numerical solution of linear
partial differential equations (PDEs), where we discretize in space and arrive
at a system of ordinary differential equations (ODEs) in time,
%
\begin{align*}
M\mathbf{u}'(t) =  \mathcal{L}(t)\mathbf{u} + \mathbf{f}(t)
	\quad\text{in }(0,T], \quad \mathbf{u}(0) = \mathbf{u}_0,
\end{align*}
%
where $M$ is a mass matrix, $\mathcal{L}(t)\in\mathbb{R}^{N\times N}$ a discrete
linear operator, and $\mathbf{f}(t)$ a time-dependent forcing
function.\footnote{Note,
PDEs with an algebraic constraint, for example, the divergence-free
constraint in incompressible Navier Stokes, instead yield a differential algebraic equation (DAE), which
requires separate careful treatment and will be the subject of a forthcoming paper.}
Then, consider time propagation using an $s$-stage Runge-Kutta scheme,
characterized by the Butcher tableau
%
\begin{align*}
	\renewcommand\arraystretch{1.2}
	\begin{array}
	{c|c}
	\mathbf{c}_0 & A_0\\
	\hline
	& \mathbf{b}_0^T
	\end{array},
\end{align*}
%
with Runge-Kutta matrix $A_0 = (a_{ij})$, weight vector $\mathbf{b}_0^T = (b_1,
\ldots, b_s)^T$, and quadrature nodes $\mathbf{c}_0 = (c_0, \ldots, c_s)$.

Runge-Kutta methods update the solution using a sum over stage vectors,
%
\begin{align}\label{eq:update}
\mathbf{u}_{n+1} & = \mathbf{u}_n + \delta t \sum_{i=1}^s b_i\mathbf{k}_i, \\
M\mathbf{k}_i & = \mathcal{L}(t_n+\delta tc_i)\cdot
	\left(\mathbf{u}_n + \delta t\sum_{j=1}^s a_{ij}\mathbf{k}_j\right) +
	\mathbf{f}(t_n+\delta tc_i).\label{eq:stages}
\end{align}
%
The stage vectors $\{\mathbf{k}_i\}$ can then be expressed as the solution of the block linear
system,
%
\begin{align}\label{eq:k0}
\left( \begin{bmatrix} M  & & \mathbf{0} \\ & \ddots \\ \mathbf{0} & & M\end{bmatrix}
	- \delta t \begin{bmatrix} a_{11}\mathcal{L}_1 & ... & a_{1s}\mathcal{L}_1 \\
	\vdots & \ddots & \vdots \\ a_{s1}\mathcal{L}_s & ... & a_{ss} \mathcal{L}_s \end{bmatrix} \right)
	\begin{bmatrix} \mathbf{k}_1 \\ \vdots \\ \mathbf{k}_s \end{bmatrix}
& = \begin{bmatrix} \mathbf{f}_1 \\ \vdots \\ \mathbf{f}_s \end{bmatrix},
\end{align}
%
where $\mathcal{L}_i := \mathcal{L}(t_n+\delta tc_i)$ and $\mathbf{f}_i :=
\mathbf{f}(t_n+\delta tc_i)$. Primarily this paper
focuses on spatial operators $\mathcal{L}$ that are independent of time; however, some of
the results hold for commuting operators, such as those that may arise from, for example,
a time-dependent reaction term, so for now we maintain this generality.

The difficulty in fully implicit Runge-Kutta methods (which we will denote IRK) lies in
solving the $Ns\times Ns$ block linear system in \eqref{eq:k0}. This paper focuses on the
parallel simulation of numerical PDEs, where $N$ is typically very large
and $\mathcal{L}$ is highly ill-conditioned. In such cases, direct
solution techniques to solve \eqref{eq:k0} are not a viable option, and fast, parallel
iterative methods must be used. However, IRK methods are rarely employed in practice due
to the difficulties of solving \eqref{eq:k0}. Even for relatively simple
parabolic PDEs where $-\mathcal{L}$ is symmetric positive definite (SPD), \eqref{eq:k0}
instead yields a large nonsymmetric system with significant block coupling. For
nonsymmetric matrices $\mathcal{L}$ that already have variable coupling, fast iterative
methods are even less likely to yield acceptable performance in solving \eqref{eq:k0}.

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{Discontinuous Galerkin in time}\label{sec:intro:dg}

Discontinuous Galerkin (DG) in time discretizations of systems of linear
ODEs give rise to linear algebraic systems of the form
\begin{equation} \label{eq:dg-in-time}
	\left( \begin{bmatrix}
		\delta_{11} M  & & \delta_{1s} M \\
		& \ddots \\
		\delta_{s1} M & & \delta_{ss} M
	\end{bmatrix}
	- \delta t \begin{bmatrix}
		m_{11}\mathcal{L}_1 & ... & m_{1s}\mathcal{L}_1 \\
		\vdots & \ddots & \vdots \\
		m_{s1}\mathcal{L}_s & ... & m_{ss} \mathcal{L}_s
	\end{bmatrix} \right)
		\begin{bmatrix} \mathbf{u}_1 \\ \vdots \\ \mathbf{u}_s \end{bmatrix}
		= \begin{bmatrix} \mathbf{r}_1 \\ \vdots \\ \mathbf{r}_s \end{bmatrix}.
\end{equation}
The coefficients $m_{ij}$ correspond to a temporal mass matrix, the coefficients
$\delta_{ij}$ correspond to a DG weak derivative with upwind numerical flux, and
the unknowns $\mathbf{u}_i$ are the coefficients of the polynomial expansion of
the approximate solution (for example, see \cite{hn,Akrivis2011,Lasaint1974,Makridakis2006}).
Both of the coefficient matrices $\{m_{ij}\}$ and $\{\delta_{ij}\}$ are
invertible. It can be seen that the algebraic form of the DG in time
discretization is closely related to the implicit Runge-Kutta system
\eqref{eq:k0} and, in fact, \eqref{eq:dg-in-time} can be recast in the form of
\eqref{eq:k0} using the invertibility of the matrix $\{\delta_{ij}\}$. In
particular, the degree-$p$ DG method using $(p+1)$-point
Radau quadrature, which is exact for polynomials of degree $2p$, is equivalent
to the Radau IIA collocation method \cite{Makridakis2006}, which is used for
many of the numerical results in \Cref{sec:numerics}.
Thus, although the remainder of this paper focuses on fully implicit Runge-Kutta,
the algorithms developed here can also be applied to DG discretizations in time on
fixed slab-based meshes.

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{Outline}\label{sec:intro:outline}

This paper develops fast, parallel preconditioning techniques for the solution of fully implicit
Runge-Kutta methods and DG discretizations in time for linear numerical PDEs. First,
\Cref{sec:intro:hist} provides background on why IRK methods are desirable over the simpler and more
commonly used digonally implicit Runge-Kutta (DIRK) methods, and also provides some historical
context for the preconditioners developed in this work. \Cref{sec:intro:stab} then briefly discusses
stable integration from a method-of-lines perspective and introduces two key elements that will be
used throughout the paper.

\Cref{sec:solve} introduces a new method to solve for the IRK update
\eqref{eq:update} for linear operators $\mathcal{L}$ that are independent of time.
This new method requires the preconditioning of $s$ real-valued matrices of the form
$\eta M - \delta t\mathcal{L}$ for some $\eta > 1$, analogous as the matrices that
arise in backward Euler integration, and is easily implemented
using existing preconditioners and parallel software libraries.
In contrast to other works that have considered the preconditioning of \eqref{eq:kron1},
the proposed algorithm here (i) is amenable to short-term Krylov recursion (conjugate
gradient (CG)/MINRES) if $\eta M - \mathcal{L}$ is, and (ii) only operates on the solution,
thus not requiring the storage of each stage vector. Theory is developed in
\Cref{sec:solve:prec} that guarantees $\mathcal{O}(1)$ conditioning of the
preconditioned operator under basic assumptions on stability from
\Cref{sec:intro:stab}, with only weak dependence on the number of
stages or polynomial order.

Numerical results are provided in \Cref{sec:numerics}, demonstrating the new
method for a variety of problems and corresponding preconditioners, including
very high-order finite-difference and discontinuous Galerkin spatial discretizations of
advection-diffusion equations, and matrix-free continuous Galerkin
discretizations of diffusion equations. The method is shown to be fast and scalable
up to 10th-order accuracy in time, effective on fully advective (hyperbolic)
problems, and, for multiple examples, can obtain 4th-order accuracy with Gauss
integration using roughly half as many preconditioning iterations
as needed by standard 4th-order SDIRK schemes.

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{Why fully implicit and previous work}\label{sec:intro:hist}

Diagonally implicit Runge-Kutta (DIRK) methods, where $A_0$ is lower triangular,
are commonly used in practice.
For such schemes, the solution of \eqref{eq:k0} using a block substitution algorithm
requires only $s$ linear
solves of systems of the form $ M - \delta ta_{ii}\mathcal{L}_i$. Unfortunately, DIRK
schemes suffer from order reduction, where the order of accuracy observed in practice
on stiff nonlinear PDEs or DAEs can be limited to $\approx \min\{ p, q+1\}$ or $q$,
respectively, for integration order $p$ and stage-order $q$ \cite{hairer96,kennedy16}.
The stage-order of a DIRK method is at most one (EDIRK methods, with one explicit stage,
have a maximum stage order of two) \cite{butcher00} and, thus, even a
6th-order DIRK method may only yield 1st- or 2nd-order accuracy. In contrast, IRK
methods may have arbitrarily high stage order and, thus, formally high-order accuracy on stiff,
nonlinear problems, and even index-2 DAEs. Although the focus of this paper is
linear PDEs without algebraic constraints, we want to highlight that the theory
and framework developed here is fundamental to a forthcoming paper on
nonlinear PDEs and DAEs. Furthermore, for less stiff
problems, IRK methods can yield accuracy as high as order $2s$ for an $s$-stage method,
compared with a maximum of $s$ or $s+1$ for SDIRK methods with reasonable stability
properties \cite[Section IV.6]{hairer96}.

One simplification for using IRK methods is to assume $\mathcal{L}_i =
\mathcal{L}_j$ for all $i,j$, in which case \eqref{eq:k0} can be expressed in
Kronecker product form,
%
\begin{align}\label{eq:kron1}
(I\otimes M - \delta t A_0\otimes \mathcal{L})\mathbf{k} & = \mathbf{f}.
\end{align}
%
Such an assumption is natural for linear problems with no time-dependent differential
components and, for the most part, covers the type of problems studied in this paper.
Many papers have considered the solution of \eqref{eq:kron1}, with Butcher
\cite{butcher76} and Bickart's \cite{bickart77} being some of the earliest works,
which develop ways to transform \eqref{eq:kron1} to a simpler form.\footnote{In Kronecker form \eqref{eq:kron1}, SIRK methods \cite{norsett1976runge} are also
relatively straightforward to solve using existing preconditioning techniques.
But, although SIRK method offer some advantages over DIRK methods, they still lack
the favorable stability and accuracy properties of IRK methods \cite{burrage82,orel91}.}
There, and in many of the works that followed, the goal was to minimize the cost of LU
decompositions used to solve \eqref{eq:kron1}, typically in the context of ODEs.

For large-scale simulation of PDEs, particularly on modern computing architectures,
LU decompositions (or other direct factorizations) are typically not feasible. In this
vein, a number of people have considered preconditioning techniques for \eqref{eq:kron1}
or approximations to \eqref{eq:kron1} on the nonlinear iteration level or time
discretization level. Various block preconditioning/approximation techniques
have been studied, primarily for parabolic problems
\cite{houwen97b,Houwen97c,nissen11,mardel07,staff06,hoffmann97,jay00}, and
multigrid methods for IRK and parabolic problems were developed in \cite{vanlent05}.
New ADI-type preconditioners for IRK methods were developed for parabolic problems
in \cite{chen14} with spectral radius shown to be $<1$ under reasonable assumptions,
and the method extended to the viscous wave equation in first-order form in
\cite{chen16}. More recently, block-ILU preconditioners were successfully applied
to a transformed version of \eqref{eq:kron1} in \cite{pazner17} on more difficult
nonlinear compressible fluids problems.
A handful of works have also studied linear solvers for DG-in-time discretizations,
primarily for parabolic problems, including block preconditioning approaches
\cite{exh,8jp,27n}, and direct space-time multigrid methods
\cite{gander2016analysis}. In fact, some of the principles used in this paper
are similar to those used in \cite{exh} for space-time DG discretizations of
linear parabolic problems, and some of the theory derived therein is generalized
to non-parabolic/non-SPD operators in this paper.

Despite many papers considering the efficient solution of IRK/DG-in time methods,
very little has been done in the development and analysis of preconditioning techniques
for non-parabolic problems/non-SPD spatial operators, particularly methods that are
amenable to combine with existing fast, parallel preconditioners. Addressing each
of these challenges for linear PDEs is the focus of this work.

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{A preconditioning framework and stability}\label{sec:intro:stab}

Throughout the paper, we use the reformulation used in, for example,
\cite{pazner17}, where we can pull an $A_0\otimes I$ out of the
fully implicit system in \eqref{eq:k0}, yielding the equivalent problem
%
\begin{align}\label{eq:keq}
\left( A_0^{-1}\otimes M - \delta t \begin{bmatrix} \mathcal{L}_1  & \\ & \ddots \\ && \mathcal{L}_s\end{bmatrix}\right)
	(A_0\otimes I)	\begin{bmatrix} \mathbf{k}_1 \\ \vdots \\ \mathbf{k}_s \end{bmatrix}
& = \begin{bmatrix} \mathbf{f}_1 \\ \vdots \\ \mathbf{f}_s \end{bmatrix}.
\end{align}
%
The off-diagonal block coupling in \eqref{eq:keq} now consists of mass matrices
rather than differential operators, which makes the analysis and solution more
tractable.
The algorithms developed here depend on the eigenvalues of $A_0$ and
$A_0^{-1}$, leading to our first assumption.
%
\begin{assumption}\label{ass:eig}
Assume that all eigenvalues of $A_0$ (and equivalently $A_0^{-1})$ have positive real part.
\end{assumption}
%
Recall that if an IRK method is A-stable, irreducible, and $A_0$ is invertible
(which includes DIRK, Gauss, Radau IIA, and Lobatto IIIC integration, among others),
then \Cref{ass:eig} holds \cite{hairer96}; that is, this assumption is
straightforward to satisfy in practice.

Stability must be taken into consideration when applying ODE solvers within a
method-of-lines approach to numerical PDEs. The Dalhquist test problem extends
naturally to this setting, where we are interested in the stability of the
linearized operator $\mathcal{L}$, for the ODE(s)
$\mathbf{u}'(t) = \mathcal{L}\mathbf{u}$, with solution $e^{t\mathcal{L}}\mathbf{u}$.
A necessary condition for stability is that the eigenvalues of $\mathcal{L}$
lie within distance $\mathcal{O}(\delta t)$ of the region of stability for
the Runge-Kutta scheme of choice (e.g., see \cite{reddy92}). Here we are
interested in implicit schemes and, because the majority of implicit Runge-Kutta schemes
used in practice are A- or L-stable, an effectively necessary condition for
stability is that the eigenvalues of $\mathcal{L}$ be nonpositive. For
normal operators, this requirement ends up being a necessary and sufficient
condition for stability.

For non-normal or non-diagonalizable operators, the analysis is more complicated.
One of the best known works on the subject is by Reddy and Trefethen \cite{reddy92},
where necessary and sufficient conditions for stability are derived as the
$\varepsilon$ pseudo-eigenvalues of $\mathcal{L}$ being within
$\mathcal{O}(\varepsilon) + \mathcal{O}(\delta t)$ of the stability region
as $\varepsilon,\delta t\to 0$. Here we relax this assumption to something
that is more tractable to work with by noting that the $\varepsilon$
pseudo-eigenvalues are contained within the field of values to
$\mathcal{O}(\varepsilon)$ \cite[Eq. (17.9)]{trefethen2005spectra},
where the field of values is defined as
%
\begin{align}\label{eq:fov}
W(\mathcal{L}) := \left\{ \langle \mathcal{L}\mathbf{x},\mathbf{x}\rangle \text{ : }
	\|\mathbf{x}\| = 1 \right\}.
\end{align}
%
This motivates the following assumption for the analysis done in this paper.
%
\begin{assumption}\label{ass:fov}
Let $\mathcal{L}$ be the linear spatial operator, and assume that $W(\mathcal{L}) \leq 0$.
\end{assumption}
%
It should be noted that the field of values has an additional connection
to stability. From \cite[Theorem 17.1]{trefethen2005spectra}, we have that
$\|e^{t\mathcal{L}}\|\leq 1$ for all $t\geq 0$ if and only if $W(\mathcal{L}) \leq 0$.
This is analogous to the ``strong stability'' discussed by Leveque
\cite[Chapter 9.5]{leveque2007finite}, as opposed to the weaker (but still
sufficient) condition $\|e^{t\mathcal{L}}\|\leq C$ for all $t\geq 0$ and
some constant $C$. In practice, \Cref{ass:fov} often holds when
simulating numerical PDEs, and in \Cref{sec:solve:prec}, it is proven that
\Cref{ass:eig} and \ref{ass:fov} provide sufficient conditions to guarantee
fast Krylov convergence of the proposed methods.

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\section{Fast parallel preconditioning}\label{sec:solve}

For ease of notation, let us scale both sides of \eqref{eq:keq} by a block
diagonal operator, with diagonal blocks $M^{-1}$, and let
%
\begin{equation*}
\widehat{\mathcal{L}}_i := \delta t M^{-1}\mathcal{L}_i,
\end{equation*}
%
for $i=1,...,s$. Now let $\alpha_{ij}$ denote the $ij$-element
of $A_0^{-1}$.\footnote{Note, there are
methods with one explicit stage followed by several fully implicit stages \cite{butcher00}.
In such cases, $A_0$ is not invertible, but the explicit stage can
be eliminated from the system (by doing an explicit time step). The remaining operator
can then be reformulated as in \eqref{eq:keq}.}
Then, solving \eqref{eq:keq} can be effectively reduced to inverting the operator
%
\begin{align}\nonumber
\mathcal{M}_s & := A_0^{-1}\otimes I - \begin{bmatrix} \widehat{\mathcal{L}}_1  & \\ & \ddots \\ && \widehat{\mathcal{L}}_s\end{bmatrix} \\
& = \begin{bmatrix} \alpha_{11}I - \widehat{\mathcal{L}}_1 & \alpha_{12}I & \cdots & \alpha_{1s}I \\
	\alpha_{21}I & \alpha_{22}I - \widehat{\mathcal{L}}_2 & \cdots & \alpha_{2s}I \\
	\vdots & \vdots & \ddots & \vdots \\ \alpha_{s1}I & \cdots & \cdots & \alpha_{ss}I - \widehat{\mathcal{L}}_s \end{bmatrix}.
	\label{eq:k1}
\end{align}
%
We proceed by deriving a closed form inverse of \eqref{eq:k1}, demonstrating
how the Runge-Kutta update in \eqref{eq:update} can then be performed directly
(without forming and saving each stage vector), and developing a preconditioning
strategy to apply this update using existing preconditioners.

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{An inverse and update for commuting operators}\label{sec:solve:inv}

This section introduces a result similar to Bickart's \cite{bickart77},
but using a different framework and generalized to hold for commuting
operators. We consider $\mathcal{M}_s$ as a matrix over the commutative
ring of linear combinations of $\{I, \widehat{\mathcal{L}}\}$,
and the determinant and adjugate referred to in \Cref{lem:inv} are defined
over matrix-valued elements rather than scalars.

%
\begin{lemma}\label{lem:inv}
Let $\alpha_{ij}$ denote the $(i,j)$th entry of $A_0^{-1}$ and assume
$\{\widehat{\mathcal{L}}_i\}_{i=1}^s$ are commuting operators. Define $\mathcal{M}_s$
% \begin{align*}
% \mathcal{M}_s := \begin{bmatrix} \alpha_{11}I - \widehat{\mathcal{L}}_1 & \alpha_{12}I & ... & \alpha_{1s}I \\
% 	\alpha_{21}I & \alpha_{22}I - \widehat{\mathcal{L}}_2 & & \alpha_{2s}I \\
% 	\ddots & & \ddots & \vdots \\ \alpha_{s1}I & ... & \alpha_{s(s-1)}I & \alpha_{ss}I - \widehat{\mathcal{L}}_s \end{bmatrix},
% \end{align*}
as in \eqref{eq:k1},
let $\det(\mathcal{M}_s)$ be the determinant of $\mathcal{M}_s$,
and let $\adj(\mathcal{M}_s)$ be the adjugate of $\mathcal{M}_s$. Then, $\mathcal{M}_s$
is invertible if and only if $\det(\mathcal{M}_s)$ is invertible, and
\begin{align*}
\mathcal{M}_s^{-1} = \textnormal{diag}\left(\det(\mathcal{M}_s)^{-1}\right)
	\textnormal{adj}(\mathcal{M}_s).
\end{align*}
%
where ``diag'' indicates a block diagonal matrix, with diagonal blocks in this
case given by $\det(\mathcal{M}_s)^{-1}$.
Now, suppose $\widehat{\mathcal{L}}_i = \widehat{\mathcal{L}}_j$ for
all $i,j$, and let $P_s(x)$ be the characteristic polynomial of $A_0^{-1}$. Then,
\begin{align*}
\mathcal{M}_s^{-1} = \textnormal{diag}(P_s(\widehat{\mathcal{L}})^{-1})\textnormal{adj}(\mathcal{M}_s),
\end{align*}
\end{lemma}
\begin{proof}
Notice in \eqref{eq:k1} that if $\widehat{\mathcal{L}}_i$ and $\widehat{\mathcal{L}}_j$ commute for all $i,j$,
then $\mathcal{M}_s$ is a matrix over the commutative ring of linear combinations
of $I$ and $\{\widehat{\mathcal{L}}_i\}$. Let adj$(\mathcal{M}_s)$ denote the matrix adjugate. A
classical result in matrix analysis tells us that
%
\begin{align*}
\textnormal{adj}(\mathcal{M}_s)\mathcal{M}_s = \mathcal{M}_s\textnormal{adj}(\mathcal{M}_s)
	= \det(\mathcal{M}_s)I.
\end{align*}
\OK{The RHS should be $\textnormal{diag}(\det(\mathcal{M}_s))I$?}
%
Moreover, $\mathcal{M}_s$ is invertible if and only if the determinant of $\mathcal{M}_s$
is invertible, in which case $\mathcal{M}_s^{-1} := $ det$(\mathcal{M}_s)^{-1}$adj$(\mathcal{M}_s)$ 
\OK{This eq. should be $\mathcal{M}_s^{-1} = \textnormal{diag}(\det(\mathcal{M}_s)^{-1})\textnormal{adj}(\mathcal{M}_s)$?}
\cite[Theorem 2.19 \& Corollary 2.21]{brown1993matrices}.
For the case of time-independent operators ($\widehat{\mathcal{L}}_i=\widehat{\mathcal{L}}_j$), notice that
$\mathcal{M}_s$ takes the form $A_0^{-1} - \widehat{\mathcal{L}}I$ over the commutative ring defined
above. Analogous to a scalar matrix, the determinant of $A_0^{-1} - \widehat{\mathcal{L}}I$ is the
characteristic polynomial of $A_0^{-1}$ evaluated at $\widehat{\mathcal{L}}$.
\end{proof}
%

Returning to \eqref{eq:keq}, we can express the direct solution for the set of all
stage vectors ${\mathbf{k}} = [\mathbf{k}_1; ...; \mathbf{k}_s]$ as
%
\begin{align*}
\mathbf{k} &:= \det(\mathcal{M}_s)^{-1}
	(A_0^{-1}\otimes I)\textnormal{adj}(\mathcal{M}_s)\mathbf{f},
\end{align*}
\OK{This eq. should have $\textnormal{diag}(\det(\mathcal{M}_s)^{-1})$?}
%
where $\mathbf{f} = [\mathbf{f}_1; ...; \mathbf{f}_s]$ (note that
$A_0\otimes I$ commutes with $\det(\mathcal{M}_s)^{-1}$ 
\OK{Then this should be ``note that
$A_0\otimes I$ commutes with $\textnormal{diag}(\det(\mathcal{M}_s)^{-1})$''}
). Excusing the slight
abuse in notation, let $\det(\mathcal{M}_s)^{-1}$ now denote just the diagonal
block (rather than a block-diagonal matrix). 
\OK{This will no longer be an abuse of notation.} 
The Runge-Kutta update is then given by
%
\begin{align}\nonumber
\mathbf{u}_{n+1} & = \mathbf{u}_n + \delta t\sum_{i=1}^s b_i{\mathbf{k}}_i \\
& = \mathbf{u}_n + \delta t\det(\mathcal{M}_s)^{-1}
	(\mathbf{b}_0^TA_0^{-1}\otimes I)\textnormal{adj}(\mathcal{M}_s)\mathbf{f}.\label{eq:update2}
\end{align}
%

\begin{remark}[Implementation \& complexity]
The adjugate consists of linear combinations of $I$ and $\widehat{\mathcal{L}}$, and an
analytical form can be derived for an arbitrary $s\times s$ matrix, where
$s\sim\mathcal{O}(1)$.
% Computing this operator analytically is easiest using a computer algebra program
% such as Mathematica.
Applying its action requires a set of vector summations
and matrix-vector multiplications. In particular, the diagonal elements of
$\textnormal{adj}(\mathcal{M}_s)$ are monic polynomials in $\widehat{\mathcal{L}}$ of
degree $s-1$ (or linear combinations of comparable degree if
$\widehat{\mathcal{L}}_i\neq\widehat{\mathcal{L}}_j$)
and off-diagonal terms are polynomials in $\widehat{\mathcal{L}}$ of degree $s-2$.

Returning to \eqref{eq:update2}, we consider two cases. First, if a given Runge-Kutta
scheme is stiffly accurate (for example, Radau IIA methods),
then $\mathbf{b}_0^TA_0^{-1} = [0,...,0,1]$. This yields
the nice simplification that computing the update in \eqref{eq:update2} only requires
applying the last row of $\textnormal{adj}(\mathcal{M}_s)$ to $\mathbf{f}$ (in a
dot-product sense) and applying $\det(\mathcal{M}_s)^{-1}$ to the result. From
the discussion above regarding the adjugate structure, applying the last row of
$\textnormal{adj}(\mathcal{M}_s)$ requires $(s-2)(s-1) + (s-1) = (s-1)^2$ matrix-vector
multiplications. Because this only happens once, followed by the linear solve(s),
these multiplications are typically of relatively marginal cost.

In the more general case of non stiffly accurate methods (for example, Gauss integration),
one can obtain an analytical form for
$(\mathbf{b}_0^TA_0^{-1}\otimes I)\textnormal{adj}(\mathcal{M}_s)$. Each element in
this block $1\times s$ matrix consists of polynomials in $\widehat{\mathcal{L}}$ of
degree $s-1$ (although
typically not monic). Compared with stiffly accurate schemes, this now requires
$(s-1)s$ matrix-vector multiplications, which is $s-1$ more than for stiffly
accurate schemes, but still typically of marginal overall computational cost.
\end{remark}

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{Preconditioning by conjugate pairs}\label{sec:solve:prec}

Following the discussion and algorithm developed in \Cref{sec:solve:inv}, the key
outstanding point is inverting $\det(\mathcal{M}_s)^{-1}$. Moving forward, we
restrict our attention to the case $\widehat{\mathcal{L}}_i = \widehat{\mathcal{L}}_j$ for all $i,j$,
in which case we have a simple closed form for $\det(\mathcal{M}_s)^{-1} =
P_s(\widehat{\mathcal{L}})^{-1}$, where $P_s(x)$ is the characteristic polynomial
of $A_0^{-1}$ (see \Cref{lem:inv}).

In contrast to much of the original work on solving IRK systems, where LU factorizations
were the dominant cost and system sizes relatively small, explicitly forming and inverting
$P_s(\widehat{\mathcal{L}})$ for numerical PDEs is typically not a viable option in high-performance
simulation on modern computing architectures. Instead, by computing the eigenvalues
$\{\lambda_i\}$ of $A_0^{-1}$, we can express $P_s(\widehat{\mathcal{L}})$ in a factored form,
%
\begin{align}\label{eq:fac}
P_s(\widehat{\mathcal{L}}) = \prod_{i=1}^s (\lambda_i I - \widehat{\mathcal{L}}),
\end{align}
%
and its inverse can then be computed by successive applications of
$(\lambda_iI - \widehat{\mathcal{L}})^{-1}$,
for $i=1,...,s$. Unfortunately, eigenvalues of $A_0$ and $A_0^{-1}$ are often
complex, and for real-valued matrices this makes the inverse of individual factors
$(\lambda_iI - \widehat{\mathcal{L}})^{-1}$ more difficult and often impractical
with standard preconditioners and existing software. Moving forward, let
$\lambda := \eta \pm \mathrm{i}\beta$ denote an eigenvalue of $A_0^{-1}$ 
\OK{This  is technically 2 eigenvalues of $A_0^{-1}$. I think it's OK to just introduce an eigenvalue as $\lambda := \eta + \mathrm{i}\beta$?}
,
for $\eta, \beta \in \mathbb{R}$, with $\beta \geq 0$ and $\eta > 0$ under \Cref{ass:eig}.

Here, we combine conjugate eigenvalues into quadratic polynomials
that we must precondition, which take the form 
\OK{The imaginary unit ``$i$'' you use below (and in the rest of the paper) is different to the ``$\mathrm{i}$'' you  use above for $\lambda$.}
%
\begin{align}\label{eq:imag1}
\begin{split}
\mathcal{Q}_\eta :&= ((\eta + i\beta)I - \widehat{\mathcal{L}})((\eta - i\beta)I - \widehat{\mathcal{L}}) \\
& = (\eta^2+\beta^2)I - 2\eta \widehat{\mathcal{L}} + \widehat{\mathcal{L}}^2
= (\eta I - \widehat{\mathcal{L}})^2 + \beta^2I.
\end{split}
\end{align}
%
In practice, we typically do not want to directly form or precondition a quadratic
operator like \eqref{eq:imag1}, due to (i) the overhead cost of large parallel matrix
multiplication, and (ii) the fact that many fast parallel methods such as multigrid are not well-suited for solving a polynomial in $\widehat{\mathcal{L}}$. The point of \eqref{eq:imag1}
is that by considering conjugate pairs of eigenvalues, the resulting operator is real-valued.
Here we propose preconditioning \eqref{eq:imag1} with the inverse of a
real-valued quadratic polynomial, $(\gamma I - \widehat{\mathcal{L}})^2$, for
$\gamma \in\mathbb{R}^+$.

% ------------------------------------------------------------------------------------- %
\subsubsection{A first preconditioning}\label{sec:solve:prec:eta}

A natural starting point is preconditioning \eqref{eq:imag1} with the inverse of
the real-valued part of the operator, $(\eta I - \widehat{\mathcal{L}})^2$,
dropping the $+ \beta^2 I$ term, which can be applied as two successive inverses
of $(\eta I - \widehat{\mathcal{L}})$. Expanding, the preconditioned operator
then takes the form
%
\begin{align}\nonumber
\mathcal{P}_\eta & := (\eta I - \widehat{\mathcal{L}})^{-2}\left[(\eta^2+\beta^2)I - 2\eta \widehat{\mathcal{L}} + \widehat{\mathcal{L}}^2\right] \\
&\hspace{5ex} = I + \beta^2(\eta I - \widehat{\mathcal{L}})^{-2}
= I + \frac{\beta^2}{\eta^2}\left(I - \tfrac{1}{\eta}\widehat{\mathcal{L}}\right)^{-2}.\label{eq:prec1}
\end{align}
%

For $\eta > \beta$ and under the assumptions introduced in \Cref{sec:intro:stab},
the preconditioned operator in \eqref{eq:prec1} has a nicely bounded field-of-values,
which can be directly translated into guaranteed GMRES convergence bounds
(\Cref{cor:gmres}).

%
\begin{theorem}[Preconditioned field of values]\label{th:fov}
Suppose Assumptions \ref{ass:eig} and \ref{ass:fov} hold, that is, $\eta > 0$
and $W(\mathcal{L}) \leq 0$, where $W(\mathcal{L})$ is defined by \eqref{eq:fov}.
Let $\mathcal{P}_\eta$ denote the
preconditioned operator, where $((\eta + i\beta)I -
\widehat{\mathcal{L}})((\eta - i\beta)I - \widehat{\mathcal{L}})$ is
preconditioned with $(\eta I - \widehat{\mathcal{L}})^{-2}$. Then
$W(\mathcal{P}_\eta)$ is contained in a circle of radius
$\tfrac{\beta^2}{\eta^2}$ centered at 1.
\end{theorem}
\begin{proof}
For operator $B$, define the symmetric/skew-symmetric splitting
$B = B_s + B_k$, where $B_s := (B+B^T)/2$ and $B_k := (B - B^T)/2$, and
the numerical radius as $r(B) = \sup \{ |\lambda| : \lambda \in W(B) \}$. Recall
the following properties of $W(B)$ \cite{gustafson1997numerical,mees1979domains}:
%
\begin{enumerate}
	\item $W(I + B) = 1 + W(B)$.

	\item $B_s \leq 0$ in the symmetric negative semi-definite sense
	if and only if $W(B) \leq 0$.

	\item $r(B) \leq \|B\|_2$.

\end{enumerate}
%
Appealing to \eqref{eq:prec1} and property (1), $W(\mathcal{P}_\eta)
= 1 + \tfrac{\beta^2}{\eta^2}W(E)$, for error term
$E := (I - \tfrac{1}{\eta}\widehat{\mathcal{L}})^{-2}$
and real-valued constant $\beta^2/\eta^2 > 0$.
Now, note that by assumption $W(\mathcal{L}) \leq 0$, and property (2)
yields $(\widehat{\mathcal{L}}+\widehat{\mathcal{L}}^T)/2 \leq 0$. Then,
%
\begin{align}\label{eq:norm1}
\frac{\left\langle (I - \tfrac{1}{\eta}\widehat{\mathcal{L}})\mathbf{x},(I - \tfrac{1}{\eta}\widehat{\mathcal{L}})\mathbf{x}\right\rangle}
	{\langle\mathbf{x},\mathbf{x}\rangle}
& = 1 - \frac{\langle (\widehat{\mathcal{L}} + \widehat{\mathcal{L}}^T )
	\mathbf{x},\mathbf{x}\rangle}{\eta \langle\mathbf{x},\mathbf{x}\rangle} +
	\frac{\langle \tfrac{1}{\eta^2}\widehat{\mathcal{L}}^T\widehat{\mathcal{L}}\mathbf{x},
	\mathbf{x}\rangle}{\langle\mathbf{x},\mathbf{x}\rangle}
\geq 1
\end{align}
%
for all $\mathbf{x}\neq\mathbf{0}$. Then,
%
\begin{align*}
\|(I - \tfrac{1}{\eta}\widehat{\mathcal{L}})^{-2}\| \leq \|(I - \tfrac{1}{\eta}\widehat{\mathcal{L}})^{-1}\|^2
& = \sup_{\mathbf{x}\neq\mathbf{0}}
	\frac{\left\langle (I - \tfrac{1}{\eta}\widehat{\mathcal{L}})^{-1}\mathbf{x},
	(I - \tfrac{1}{\eta}\widehat{\mathcal{L}})^{-1}\mathbf{x}\right\rangle}
	{\langle\mathbf{x},\mathbf{x}\rangle} \\
&\hspace{-10ex}= \sup_{\mathbf{y}\neq\mathbf{0}}
	\frac{\langle\mathbf{y},\mathbf{y}\rangle}{\left\langle (I - \tfrac{1}{\eta}\widehat{\mathcal{L}})\mathbf{y},
	(I - \tfrac{1}{\eta}\widehat{\mathcal{L}})\mathbf{y}\right\rangle}
\leq 1.
\end{align*}
%
By property (3), this yields a bound on the numerical radius
$r(E) = r((I - \tfrac{1}{\eta}\widehat{\mathcal{L}})^{-2})
\leq \|(I - \tfrac{1}{\eta}\widehat{\mathcal{L}})^{-2}\|\leq 1$. It follows that
the field $W(E)$ is contained in a unit circle in the complex plane,
which completes the proof.
\end{proof}
%

%
\begin{corollary}[GMRES convergence bounds]\label{cor:gmres}
Let $\pi_k$ denote the set of polynomials of degree $k$ such that $p(0) = 1$,
and let $\beta < \eta$. Then the ideal GMRES bound (an upper bound in operator
norm of worst-case convergence) on convergence after $k$ iterations
applied to the preconditioned operator $\mathcal{P}_\eta$ \eqref{eq:prec1}
is bounded by
\begin{align*}
\min_{p\in\pi_k} \|p(\mathcal{P}_\eta)\| \leq 2\left(\frac{\beta^2}{\eta^2}\right)^k.
\end{align*}
\end{corollary}
\begin{proof}
For operator $B$, let $\nu(B)$ denote the distance of $W(B)$ from the origin and define
$\cos(\zeta) := \nu(B) / r(B)$. In \cite{liesen2012field}, convergence
estimates are derived for GMRES based on the field of values. Combining Equations (3.6)
and (3.9) in \cite{liesen2012field} yields a worst-case convergence bound for GMRES
applied to operator $B$ as
%
\begin{align}\label{eq:gmres}
\min_{p\in\pi_k} \|p(B)\| \leq 2\left(\frac{1-\cos\zeta}{1+\cos\zeta}\right)^k.
\end{align}
%
From \Cref{th:fov}, we have $\nu(\mathcal{P}_\eta) \geq 1-\beta^2/\eta^2$ and
$r(\mathcal{P}_\eta) \leq 1+\beta^2/\eta^2$. Plugging into
\eqref{eq:gmres} completes the proof.
\end{proof}
%

Making the additional assumption that
$W((I - \tfrac{1}{\eta}\widehat{\mathcal{L}})^2) \geq 0$
(or a stronger assumption that $W(\widehat{\mathcal{L}}^2) \geq 0$),
\Cref{th:fov} can be extended to bound $W(\mathcal{P}_\eta)$
in the \textit{positive} half circle of radius $\beta^2/\eta^2$, centered at 1. Then,
$\nu(\mathcal{P}_\eta)\geq 1$ and $r(\mathcal{P}_\eta) \leq 1+\beta^2/\eta^2$,
and GMRES bounds can be derived as in \Cref{cor:gmres}, independent of $\beta$
and $\eta$, as
%
\begin{align}\label{eq:gmres2}
\min_{p\in\pi_k} \|p(\mathcal{P}_\eta)\| \leq 2\left(\frac{\beta^2/\eta^2}
	{2 + \beta^2/\eta^2}\right)^k.
\end{align}
%
Such an assumption is tricky to quantify because there is no necessary relationship
between $W(A)$ and $W(A^2)$ for operator $A$. Roughly,
$W(\widehat{\mathcal{L}}^2)\geq 0$ would correspond to parabolic-type
operators, where eigenvalues have larger real than imaginary components.
In any case, notice that even there, the field-of-values and GMRES convergence 
estimates \eqref{eq:gmres2} depend on the ratio $\beta^2/\eta^2$. For
$\eta < \beta$, we expect to see nice convergence, but are likely to see degradation
for $\beta > \eta$. The following section derives a better constant $\gamma$
that yields nice conditioning regardless of $W(\widehat{\mathcal{L}}^2)$,
and with weaker dependence on $\eta$ and $\beta$.

% ------------------------------------------------------------------------------------- %
\subsubsection{A better choice of $\gamma$}\label{sec:solve:prec:gamma}

Consider a similar preconditioner as in
\Cref{sec:solve:inv}, but with preconditioning constant $\gamma \mapsto
(\gamma I - \widehat{\mathcal{L}})^{-2}$. The resulting preconditioned
operator takes the form
%
\begin{align}\nonumber
\mathcal{P}_\gamma & =
(\gamma I - \mathcal{L})^{-2}\Big[(nI - \mathcal{L})^2 + \beta^2 I\Big] \\ \nonumber
& = (\gamma I - \mathcal{L})^{-2}\Big[((\eta-\gamma)I + (\gamma I - \mathcal{L}))^2 + \beta^2 I\Big] \\
% & = (\gamma I - \mathcal{L})^{-2}\Big[(\gamma-\eta)^2I - 2(\gamma-\eta)(\gamma I - \mathcal{L}) +
% 	(\gamma I - \mathcal{L})^2 + \beta^2 I\Big] \nonumber\\
% & = I - 2(\gamma-\eta)(\gamma I - \mathcal{L})^{-1} + (\beta^2 + (\gamma-\eta)^2)(\gamma I -
% 	\mathcal{L})^{-2} \nonumber\\
& = I - 2\frac{\gamma-\eta}{\gamma}\left(I - \tfrac{1}{\gamma}\mathcal{L}\right)^{-1} +
	\frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2}
	\left(I - \tfrac{1}{\gamma}\mathcal{L}\right)^{-2}.\label{eq:prec_k}
\end{align}
%
Note that in \eqref{eq:prec_k} we have a quadratic polynomial in
$(I - \tfrac{1}{\gamma}\mathcal{L})^{-1}$. Although this provides
nice structure, the field of values analysis applied in \Cref{sec:solve:inv}
becomes much more complicated because there is no necessary relationship between
$\langle A\mathbf{x},\mathbf{x}\rangle$ and
$\langle A^2\mathbf{x},\mathbf{x}\rangle$ for general operators $A$. Thus, here
we take a different approach, analyzing the condition number of the preconditioned
operator, $\mathcal{P}_\gamma$, similar to as done for SPD matrices in \cite{exh}.

% %
% Under the additional assumption of $\widehat{\mathcal{L}}^2 \geq 0$, $\gamma = \eta$
% yields bounds
% %
% \begin{align*}
% \textnormal{cond}(\mathcal{P}_{\gamma_*}) \leq 1 + \frac{\beta^2}{\eta^2}.
% \end{align*}
% %

\begin{theorem}[Conditioning of preconditioned operator]\label{th:cond}
Suppose Assumptions \ref{ass:eig} and \ref{ass:fov} hold, that is, $\eta > 0$
and $W(\widehat{\mathcal{L}}) \leq 0$ \eqref{eq:fov}. Let $\mathcal{P}_\gamma$ denote
the preconditioned operator, where $((\eta + i\beta)I -
\widehat{\mathcal{L}})((\eta - i\beta)I - \widehat{\mathcal{L}})$ is
preconditioned with $(\gamma I - \widehat{\mathcal{L}})^{-2}$.
Define $\gamma_* = \sqrt{\eta^2+\beta^2}$ and let $\gamma = \gamma_*$. Then
%
\begin{align}\label{eq:gammastar}
\textnormal{cond}(\mathcal{P}_{\gamma_*}) \leq 
	3\sqrt{1+\frac{\beta^2}{\eta^2}} - 2.
\end{align}

Assume additionally that $W(\widehat{\mathcal{L}}^2) \geq 0$. Then for
$\eta \leq \gamma \leq \gamma_*$ 
\begin{align}\label{eq:cond_gen}
\textnormal{cond}(\mathcal{P}_\gamma) \leq \frac{\gamma}{\eta}
	\left(1 + \frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2}\right).
\end{align}
\end{theorem}
\begin{proof}
Recall for matrix $A$, cond$(A) = \|A\|\|A^{-1}\|$.
First, consider bounding $\|\mathcal{P}_\gamma\|$ for $\gamma \geq \eta$:
%
\begin{align}\nonumber
\|\mathcal{P}_\gamma\| & = \left\| I - 2\frac{\gamma-\eta}
	{\gamma}\left(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}}\right)^{-1} +
		\frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2}
		\left(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}}\right)^{-2} \right\| \\
% & \leq \left\| I - 2\frac{\gamma-\eta}
% 	{\gamma}\left(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}}\right)^{-1}\right\| +
% 		\frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2}\left\|
% 		\left(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}}\right)^{-2} \right\| \\
& \leq \left\| I - 2\frac{\gamma-\eta}
	{\gamma}\left(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}}\right)^{-1}\right\| +
		\frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2}\left\|
		\left(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}}\right)^{-1} \right\|^2 \nonumber \\
& \leq \left\| I - 2\frac{\gamma-\eta}
	{\gamma}\left(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}}\right)^{-1}\right\| +
		\frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2}. \label{eq:P0}
\end{align}
%
For the first term, note that maximizing over $\mathbf{v}\in\mathbb{R}^n$ and
letting $\mathbf{v} := (I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})\mathbf{w}$,
%
{\small
\begin{align*}
\left\| I - 2\tfrac{\gamma-\eta}
	{\gamma}(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{-1}\right\|^2
		& = \sup_{\mathbf{v}\neq\mathbf{0}} \frac{\left\| [I - 2\frac{\gamma-\eta}
	{\gamma}(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{-1}]\mathbf{v}\right\|^2}{\|\mathbf{v}\|^2} 
= \sup_{\mathbf{w}\neq\mathbf{0}} \frac{\left\| (I - \tfrac{1}{\gamma}\widehat{\mathcal{L}} -
		2\frac{\gamma-\eta}{\gamma}I )\mathbf{w}\right\|^2}{\|(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})
		\mathbf{w}\|^2} \\
% & = \sup_{\mathbf{w}\neq\mathbf{0}} \frac{\left\|[(1 - 2\frac{\gamma-\eta}{\gamma})
% 	I - \tfrac{1}{\gamma}\widehat{\mathcal{L}}]\mathbf{w}\right\|^2}{\|(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})
% 		\mathbf{w}\|^2} \\
&\hspace{-5ex} = \sup_{\mathbf{w}\neq\mathbf{0}} \frac{|1 - 2\tfrac{\gamma-\eta}{\gamma}|^2\|\mathbf{w}\|^2
	- \tfrac{1}{\gamma}(1 - 2\tfrac{\gamma-\eta}{\gamma})\langle (\widehat{\mathcal{L}} + \widehat{\mathcal{L}}^T)
		\mathbf{w},\mathbf{w}\rangle + \tfrac{1}{\gamma}^2\|\widehat{\mathcal{L}}\mathbf{w}\|^2}
	{\|\mathbf{w}\|^2 - \tfrac{1}{\gamma}\langle (\widehat{\mathcal{L}} + \widehat{\mathcal{L}}^T)
		\mathbf{w},\mathbf{w}\rangle + \tfrac{1}{\gamma}^2\|\widehat{\mathcal{L}}\mathbf{w}\|^2}.
\end{align*}
}
%
Note that by assumption, $\gamma \geq \eta$, which implies
$0 \leq 2\tfrac{\gamma-\eta}{\gamma}  < 2$, and 
$|1 - 2\tfrac{\gamma-\eta}{\gamma}| < 1$. In addition, we assume $W(\widehat{\mathcal{L}})\leq 0$,
which implies $\langle (\widehat{\mathcal{L}}+\widehat{\mathcal{L}}^T)\mathbf{w},\mathbf{w}\rangle \leq 0$
(see \Cref{th:fov}), and
$2\tfrac{\gamma-\eta}{\gamma^2} \geq 0$, which implies the term
$2\tfrac{\gamma-\eta}{\gamma^2}\langle (\widehat{\mathcal{L}}+\widehat{\mathcal{L}}^T)
	\mathbf{w},\mathbf{w}\rangle\leq 0$. Appealing to these two inequalities,
%
\begin{align} \label{eq:P1}
\left\| I - 2\tfrac{\gamma-\eta}
	{\gamma}(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{-1}\right\|^2
& < \sup_{\mathbf{w}\neq\mathbf{0}} \frac{\|\mathbf{w}\|^2
	- \tfrac{1}{\gamma}\langle (\widehat{\mathcal{L}} + \widehat{\mathcal{L}}^T)
		\mathbf{w},\mathbf{w}\rangle + \tfrac{1}{\gamma}^2\|\widehat{\mathcal{L}}\mathbf{w}\|^2}
	{\|\mathbf{w}\|^2 - \tfrac{1}{\gamma}\langle (\widehat{\mathcal{L}} + \widehat{\mathcal{L}}^T)
		\mathbf{w},\mathbf{w}\rangle + \tfrac{1}{\gamma}^2\|\widehat{\mathcal{L}}\mathbf{w}\|^2} 
= 1.
\end{align}
%
Combining \eqref{eq:P0} and \eqref{eq:P1} yields
%
\begin{align}\label{eq:Pgamma}
\|\mathcal{P}_\gamma\| \leq 1 + \frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2}.
\end{align}
\OK{But \eqref{eq:P1} says $\left\| I - 2\tfrac{\gamma-\eta}
	{\gamma}(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{-1}\right\|^2 < 1$, so shouldn't sticking this into \eqref{eq:P0} result in a ``$<$'' rather than a ``$\leq$'' for $\|\mathcal{P}_\gamma\|$ in  \eqref{eq:Pgamma}?}

Now consider bounding $\|\mathcal{P}_\gamma^{-1}\|$ from above. Let $s_{\max}(A)$
and $s_{\min}(A)$ denote the maximum and minimum singular value of matrix $A$,
respectively, and recall
%
\begin{align*}
\|\mathcal{P}_\gamma^{-1}\| = s_{\max}(\mathcal{P}_\gamma^{-1})
	& = \frac{1}{s_{\min}(\mathcal{P}_\gamma)}, \hspace{5ex}\textnormal{where}\hspace{2ex}
s_{\min}(\mathcal{P}_\gamma) =
	\min_{\mathbf{v}\neq\mathbf{0}} \frac{\|\mathcal{P}_\gamma\mathbf{v}\|}{\|\mathbf{v}\|}.
\end{align*}
%
Thus, consider the minimum singular value of $P_\gamma$. Letting $\mathbf{v} :=
(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{2}\mathbf{w}$,
%
\begin{align}\nonumber
s_{\min}(\mathcal{P}_\gamma)^2 & = \min_{\mathbf{v}\neq\mathbf{0}}
	\frac{\left\| \left[I - 2\frac{\gamma-\eta}{\gamma}(I - \tfrac{1}{\gamma}
		\widehat{\mathcal{L}})^{-1} + \frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2}
		(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{-2}\right]\mathbf{v} \right\|^2}
	{\|\mathbf{v}\|^2} \\
& = \min_{\mathbf{v}\neq\mathbf{0}}
	\frac{\left\| \left[(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{2} - 2\frac{\gamma-\eta}{\gamma}
		(I - \tfrac{1}{\gamma} \widehat{\mathcal{L}}) + \frac{\beta^2 + (\gamma-\eta)^2}{\gamma^2} I
		\right]\mathbf{w} \right\|^2}
	{\|(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{2}\mathbf{w}\|^2} \nonumber\\
& = \min_{\mathbf{v}\neq\mathbf{0}}
	\frac{\left\| \left[(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{2} + 
		\tfrac{\eta^2+\beta^2-\gamma^2}{\gamma^2} I + 2\frac{\gamma-\eta}{\gamma^2}
		\widehat{\mathcal{L}}) \right]\mathbf{w} \right\|^2}
	{\|(I - \tfrac{1}{\gamma}\widehat{\mathcal{L}})^{2}\mathbf{w}\|^2}.\label{eq:smin0}
\end{align}
\OK{These last two equations should min over w not v?}
%

For ease of notation, define
%
\begin{align*}
c_I & := \tfrac{\eta^2+\beta^2-\gamma^2}{\gamma^2} , \hspace{5ex}
c_{\widehat{\mathcal{L}}} := 2\frac{\gamma-\eta}{\gamma^2},
\end{align*}
%
noting that for $\eta\leq\gamma\leq\sqrt{\eta^2+\beta^2}$, $c_I,c_{\widehat{\mathcal{L}}} \geq 0$.
Now, collect like terms in \eqref{eq:smin0} and expand the norm to yield
%
\begin{align}\nonumber
s_{\min}(\mathcal{P}_\gamma)^2
& = \min_{\mathbf{w}\neq\mathbf{0}}
	\frac{\left\| \left[(1+c_I)I - (\tfrac{2}{\gamma}-c_{\widehat{\mathcal{L}}})\widehat{\mathcal{L}} +
		\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\right]\mathbf{w} \right\|^2}
	{\left\|\left(I - \tfrac{2}{\gamma}\widehat{\mathcal{L}} +
		\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\right)\mathbf{w} \right\|^2} \\
& =  \min_{\mathbf{w}\neq\mathbf{0}}
	\frac{\|(1+c_I)\mathbf{w}+\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}\|^2 +
		(\tfrac{2}{\gamma}-c_{\widehat{\mathcal{L}}})^2\|\widehat{\mathcal{L}}\mathbf{w}\|^2}
	{\|\mathbf{w}+\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}\|^2 +
		\tfrac{4}{\gamma^2}\|\widehat{\mathcal{L}}\mathbf{w}\|^2 -
		\tfrac{4}{\gamma}\langle\widehat{\mathcal{L}}\mathbf{w},\mathbf{w}\rangle -
		\tfrac{4}{\gamma^3}\langle\widehat{\mathcal{L}}(\widehat{\mathcal{L}}\mathbf{w}),\widehat{\mathcal{L}}\mathbf{w}\rangle} + ... \nonumber\\
& \hspace{10ex} \frac{-2(1+c_I)(\tfrac{2}{\gamma}-c_{\widehat{\mathcal{L}}})
	\langle\widehat{\mathcal{L}}\mathbf{w},\mathbf{w}\rangle -
		2\tfrac{2-\gamma c_{\widehat{\mathcal{L}}}}{\gamma^3}
		\langle\widehat{\mathcal{L}}(\widehat{\mathcal{L}}\mathbf{w}),
			\widehat{\mathcal{L}}\mathbf{w}\rangle}
	{\|\mathbf{w}+\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}\|^2 +
		\tfrac{4}{\gamma^2}\|\widehat{\mathcal{L}}\mathbf{w}\|^2 -
		\tfrac{4}{\gamma}\langle\widehat{\mathcal{L}}\mathbf{w},\mathbf{w}\rangle -
		\tfrac{4}{\gamma^3}\langle\widehat{\mathcal{L}}(\widehat{\mathcal{L}}\mathbf{w}),\widehat{\mathcal{L}}\mathbf{w}\rangle}.\label{eq:smin_ex}
\end{align}
%
By assumption on $W(\widehat{\mathcal{L}})\leq 0$ and $\eta\leq\gamma\leq\sqrt{\eta^2+\beta^2}$,
all terms in the numerator and denominator are non-negative (note, $\mathbf{w}$ and
$\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}$ were kept together in norm to avoid
the difficulties that arise in analyzing $\langle\widehat{\mathcal{L}}^2\mathbf{w},\mathbf{w}\rangle$). For numerator $\mathcal{N}$ and denominator $\mathcal{D}$, if $\widehat{K}\mathcal{N}
\geq \mathcal{D}$, then $\mathcal{N}/\mathcal{D} \geq 1 / \widehat{K}$. 
\OK{This step assumes that ${\cal D} \neq 0$, but this is not necessarily true since some of the terms are only non-negative rather than positive? Does this matter?} 
For 
$\|\widehat{\mathcal{L}}\mathbf{w}\|^2$, $\langle\widehat{\mathcal{L}}\mathbf{w},\mathbf{w}\rangle$,
and $\langle\widehat{\mathcal{L}}(\widehat{\mathcal{L}}\mathbf{w}),\widehat{\mathcal{L}}\mathbf{w}\rangle$,
we can directly bound the ratio of constants in the numerator and denominator.
For $\|\widehat{\mathcal{L}}\mathbf{w}\|^2$, we have
%
\begin{align*}
\widehat{K}(\tfrac{2}{\gamma}-c_{\widehat{\mathcal{L}}})^2 \geq \frac{4}{\gamma^2}
	\hspace{5ex}\Longleftrightarrow\hspace{5ex}
\widehat{K} \geq \frac{\gamma^2}{\eta^2},
\end{align*}
%
while $\langle\widehat{\mathcal{L}}\mathbf{w},\mathbf{w}\rangle$ requires
%
\begin{align*}
2\widehat{K}(1+c_I)(\tfrac{2}{\gamma}-c_{\widehat{\mathcal{L}}})\geq \frac{4}{\gamma},
	\hspace{5ex}\Longleftrightarrow\hspace{5ex}
\widehat{K} \geq \frac{\gamma^3}{\eta(\eta^2+\beta^2)},
\end{align*}
%
and $\langle\widehat{\mathcal{L}}(\widehat{\mathcal{L}}\mathbf{w}),\widehat{\mathcal{L}}\mathbf{w}\rangle$
requires
%
\begin{align*}
2\widehat{K}\tfrac{2-\gamma c_{\widehat{\mathcal{L}}}}{\gamma^3} \leq \frac{4}{\gamma^3},
	\hspace{5ex}\Longleftrightarrow\hspace{5ex}
\widehat{K} \geq \frac{\gamma}{\eta}.
\end{align*}
%
Because we want a lower bound on $\mathcal{N}/\mathcal{D} \geq 1/\widehat{K}$,
we seek the maximum required $\widehat{K}$ of the above ratios. For
$\gamma \in[\eta,\sqrt{\eta^2+\beta^2}]$, the maximum of the
above ratios ends up being $\widehat{K} := \tfrac{\gamma^2}{\eta^2}$.

The difficult ratio to bound in \eqref{eq:smin_ex} ends up being 
$\|(1+c_I)\mathbf{w}+\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}\|^2$ and the
similar denominator term, $\|\mathbf{w}+\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}\|^2$.
Here we consider two cases:\\
\\
%%%%%
\underline{$\boldsymbol{\gamma_* = \sqrt{\eta^2+\beta^2}}$}: Under this specific choice, we
get the nice cancellation that $c_I = 0$, and have matching terms
$\|\mathbf{w}+\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}\|^2$ in the numerator
and denominator of \eqref{eq:smin_ex}. Then, the maximum ratio of numerator 
to denominator terms is $\widehat{K} := \tfrac{\gamma^2}{\eta^2}$ derived above,
and we can bound $\|P_{\gamma}^{-1}\|$ via
%
\begin{align}\label{eq:gammas_bound}
\|P_{\gamma}^{-1}\| & = \frac{1}{s_{\min}(P_{\gamma}^{-1})} 
	\leq \frac{1}{1/\sqrt{\widehat{K}}} 
	= \frac{\sqrt{\eta^2+\beta^2}}{\eta} =
	\sqrt{1+\frac{\beta^2}{\eta^2}}.
\end{align}
\OK{Shouldn't this read $\|P_{\gamma}^{-1}\| = \frac{1}{s_{\min}(P_{\gamma})}$?}
%
Combining with \eqref{eq:Pgamma} for $\gamma = \gamma_*$ and simplifying
completes the proof of \eqref{eq:gammastar}.\\
\\
%%%%%
\noindent
\underline{\textbf{Nonnegative $\widehat{\mathcal{L}}^2$}}: Suppose
$W(\widehat{\mathcal{L}}^2) \geq 0$.
Given we are minimizing over real vectors, we can write $\langle\widehat{\mathcal{L}}^2\mathbf{w},
\mathbf{w} \rangle := \cos(\theta)\|\widehat{\mathcal{L}}^2\mathbf{w}\|\|\mathbf{w}\|$ for some
angle $\theta$. By assumption $W(\widehat{\mathcal{L}}^2) \geq 0$, we have $\cos(\theta)\geq 0$.
Taking the ratio of these individual terms then yields
%
{\small
\begin{align}\label{eq:bad}
\frac{\|(1+c_I)\mathbf{w}+\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}\|^2}
	{\|\mathbf{w}+\tfrac{1}{\gamma^2}\widehat{\mathcal{L}}^2\mathbf{w}\|^2}
& = \frac{(1+c_I)^2\|\mathbf{w}\|^2+\tfrac{1}{\gamma^4}\|\widehat{\mathcal{L}}^2\mathbf{w}\|^2
	+ 2\cos(\theta)(1+c_I)\|\widehat{\mathcal{L}}^2\mathbf{w}\|\|\mathbf{w}\|}
	{\|\mathbf{w}\|^2+\tfrac{1}{\gamma^4}\|\widehat{\mathcal{L}}^2\mathbf{w}\|^2
	+ 2\cos(\theta)\|\widehat{\mathcal{L}}^2\mathbf{w}\|\|\mathbf{w}\|}.
\end{align}
}
%
Letting $x := \tfrac{\|\widehat{\mathcal{L}}\mathbf{w}\|}{\|\mathbf{w}\|}$, we can
cancel $1/\|\mathbf{w}\|^2$ from the numerator and denominator and
write \eqref{eq:bad} as a ratio of quadratic functions in $x \geq 0$:
%
\begin{align}\label{eq:x_simp}
\frac{x^2 + 2\cos(\theta)(1+c_I)x + (1+c_I)^2}{x^2 + 2\cos(\theta)x + 1}.
\end{align}
%
For $c_I\geq 0$ (corresponding to $\gamma \leq\sqrt{\eta^2+\beta^2}$)
and $\cos(\theta)\geq 0$, \eqref{eq:x_simp} is $\geq 1$ for all $x$.
%
%---------------- Comment on \gamma > \sqrt(\eta^2+\beta^2) ----------------
% It is
% also clear in \eqref{eq:x_simp} why choosing $\gamma > \sqrt{\eta^2+\beta^2}
% \implies c_I < 0$ does not appear to be advantageous, at least for this term,
% as the ratio in \eqref{eq:x_simp} is then $<1$ for all $x\geq 0$, which
% requires a larger $\widehat{K}$ and, thus, smaller bound on
% $s_{\min}(\mathcal{P}_\gamma)$.
Similar to the proof for $\gamma = \gamma_*$, the ratio in \eqref{eq:bad}
is always $\geq 1$, and the maximum required $\widehat{K}$ to bound
$\widehat{K}\mathcal{N} \geq \mathcal{D}$ for each matching term is
given by $\widehat{K} = \tfrac{\gamma^2}{\eta^2}$. Altogether,
this yields
%
\begin{align*}
\|P_{\gamma}^{-1}\| & = \frac{1}{s_{\min}(P_{\gamma}^{-1})} 
	\leq \frac{1}{1/\sqrt{\widehat{K}}} 
	= \frac{\gamma}{\eta}.
\end{align*}
%
Combining with \eqref{eq:Pgamma} completes the proof of \eqref{eq:cond_gen}.
\end{proof}
%

In \cite{exh}, similar analysis is done under the assumption
that $-\mathcal{L}$ is SPD, in which case the conditioning can be derived based on
eigenvalues. There, they effectively derive the conditioning of the preconditioned
operator for $\gamma = \eta$ to be cond$(\mathcal{P}_\eta) = 1 + \tfrac{\beta^2}{\eta^2}$
(using notation of this paper, as the spectrum of $(I - \tfrac{1}{\eta}\mathcal{L})^{-1}
\mapsto [0,1]$, as would likely happen with refinement of a parabolic problem,
the result in \cite{exh} is exact). If we let $\gamma = \eta$, \Cref{th:cond} yields
%
\begin{align}\label{eq:cond_eta}
\textnormal{cond}(\mathcal{P}_{\gamma = \eta}) \leq 1 + \frac{\beta^2}{\eta^2}.
\end{align}
%
In addition, as $\beta \to 0$, $\gamma \to \eta$ and the condition number
\eqref{eq:cond_gen} $\to 1$ (as expected), and applying the general results
in \eqref{eq:cond_gen} to $\gamma_*$ exactly yields \eqref{eq:gammastar}.
Altogether, this suggests the condition number bounds in \Cref{th:cond}
are moderately tight.

One open question is whether the modified constant $\gamma_*$ (as opposed to,
for example, $\gamma=\eta$) is \textit{necessary} for problems where
$W(\widehat{\mathcal{L}}^2) \not\geq 0$. One might note that the condition
number bounds of $\mathcal{P}_\eta$ and $\mathcal{P}_{\gamma_*}$ do not differ
significantly for reasonable values of $\eta$ and $\beta$ when
$W(\widehat{\mathcal{L}}^2) \geq 0$. However, a hyperbolic
example in \Cref{sec:numerics:dg} demonstrates almost a $6\times$ reduction
in iteration count achieved by using $\gamma_*$ instead of $\eta$. For
parabolic/diffusive problems we have explored, $\gamma_*$ has offered a much
more modest improvement over $\eta$, suggesting that the modified constant
$\gamma_*=\sqrt{\eta^2+\beta^2}$ may be necessary/particularly important
for a robust method on hyperbolic or advection-dominated problems.
Furthermore, $\mathcal{P}_{\gamma_*}$ has performed better than
$\mathcal{P}_{\eta}$ for all problems we have
tested, and is the recommended preconditioner in practice.

\Cref{tab:cond} provides condition number bounds from \Cref{th:cond} and
\eqref{eq:gammastar} for Gauss, Radau IIA, and Lobatto IIIC Runge-Kutta methods.

%
{
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{table}[!ht]
  \centering
  \begin{tabular}{| c | c | cc | cc | ccc |}  % chktex 44
  \hline
Stages & 2 & \multicolumn{2}{c}{3} & \multicolumn{2}{|c}{4} & \multicolumn{3}{|c|}{5} \\\hline\hline
Gauss  & 1.46 & 1.00 & 2.15 & 2.83 & 1.13 & 1.00 & 3.50 & 1.38 \\
Radau IIA  & 1.67 & 1.00 & 2.54 & 3.37 & 1.16 & 1.00 & 4.15 & 1.45 \\
Lobatto IIIC  & 2.24 & 1.00 & 3.37 & 4.37 & 1.19 & 1.00 & 5.27 & 1.52 \\\hline
  \end{tabular}
  \caption{Bounds on cond$(\mathcal{P}_{\gamma_*})$ from \Cref{th:cond} and
  \eqref{eq:gammastar} for Gauss, Radau IIA, and Lobatto IIIC integration,
  with 2--5 stages. Each column within a given set of stage columns corresponds
  to a conjugate pair of eigenvalues of $A_0^{-1}$. For odd numbers of stages,
  one eigenvalue is real, corresponding to the column with conditioning 1.}\label{tab:cond}
\end{table}
% \beta^2/\eta^2 & 0.33 & 0 & 0.91 & 1.59 & 0.09 & 0 & 2.36 & 0.27 \\
% \beta^2/\eta^2 & 0.50 & 0 & 1.29 & 2.21 & 0.11 & 0 & 3.20 & 0.32	\\
% \beta^2/\eta^2 & 1 & 0 & 2.21 & 3.51 & 0.13 & 0 & 4.88 & 0.38 \\
% Matlab array:
% bn = [0.33 , 0 , 0.91 , 1.59 , 0.09 , 0 , 2.36 , 0.27; 0.50 , 0 , 1.29 , 2.21 , 0.11 , 0 , 3.20 , 0.32; 1 , 0 , 2.21 , 3.51 , 0.13 , 0 , 4.88 , 0.38];

}

\begin{remark}[Mass matrices]
Recall in the finite element context where mass matrices are involved, we defined
$\widehat{\mathcal{L}} := \delta t M^{-1}\mathcal{L}$. For a given conjugate pair
of eigenvalues, the quadratic polynomial \eqref{eq:imag1} can be expressed as
%
\begin{align}\label{eq:scaleM}
\mathcal{Q}_\eta = M^{-1}((\eta + i\beta)M - \delta t{\mathcal{L}})M^{-1}((\eta - i\beta)M -
	\delta t\widehat{\mathcal{L}}).
\end{align}
%
In this context, it is best to first scale both sides of the linear system by $M$.
This halves the number of times $M^{-1}$ must applied for each Krylov iteration,
and if $M$ and $\mathcal{L}$ are Hermitian, the resulting quadratic system is SPD
and can be solved using preconditioned CG or MINRES, preconditioned
with one application of a preconditioner, the action of $M$, and a second application
of the preconditioner.
\end{remark}

\begin{remark}[Inexact preconditioning]\label{sec:inexact-precond}
In practice, fully converging $(\gamma_* I - \widehat{\mathcal{L}})^{-1}$ 
each iteration as a preconditioner is often not desirable due to the cost
of performing a full linear solve. Here, we propose
applying a Krylov method to $\mathcal{Q}_\eta:=(\eta^2+\beta^2)I - 2\eta\widehat{\mathcal{L}} +
\widehat{\mathcal{L}}^2$ by computing the operator's action (that is, not fully constructing
it), and preconditioning each Krylov iteration with \textit{two} applications of a sparse
parallel preconditioner for $(\gamma_* I - \widehat{\mathcal{L}})$, approximating the action
of $(\gamma_* I - \widehat{\mathcal{L}})^{-2}$.

Analogous to standard block-preconditioning techniques, this approximate inverse
approach is often (but not always) more efficient than computing a full inverse
each iteration. However, \textit{it is important that the underlying preconditioner
provides a good approximation}. 
Fortunately, for difficult problems without highly effective
preconditioners, it is straightforward to apply either multiple inner fixed-point iterations
or an inner Krylov iteration (wrapped with a flexible outer Krylov method
\cite{Notay2000,saad1993flexible}) to ensure robust (outer) iterations.
In \Cref{sec:numerics:dg:diff}, a
numerical example is shown where the proposed method diverges using a single inner
fixed-point iteration as a preconditioner for $(\eta I - \widehat{\mathcal{L}})$, but
three (or more) inner fixed-point iterations yields fast, scalable convergence. 
\OK{I don't think the example in \Cref{sec:numerics:dg:diff} uses $\gamma = \eta$?}
\end{remark}

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\begin{comment}
\section{Implementation details:  The time-independent case}
The basic idea is that we need to do the update
\begin{align} \label{eq:RK_update}
\bm{u}_{n+1}  = \bm{u}_n + \delta t \det({\cal M}_s)^{-1} \big( \bm{b}_0^\top A^{-1}_0 \otimes I \big) \adj({\cal M}_s) \bm{f},
\end{align}
where we have
\begin{itemize}
\item $m \in \mathbb{N}$: The dimension of the spatial problem

\item $\bm{f} = (\bm{f}_1, \ldots, \bm{f}_s)^\top$, with $\bm{f}_j \in \mathbb{R}^m$

\item $\tilde{\bm{b}}^\top_0 \coloneqq \bm{b}^\top_0 A^{-1}_0 \in \mathbb{R}^{1 \times s}$ (this is a row vector, not that this distinction really matters)
\end{itemize}

Now we break \eqref{eq:RK_update} into two steps:
\begin{enumerate}
\item{Step 1:}\label{it:update_step1}Compute
\begin{align} \label{eq:step1}
\bm{z} = \big( \hat{\bm{b}}^\top_0 \otimes I \big) \adj ({\cal M}_s) \bm{f} \in \mathbb{R}^m
\end{align}

\item{Step 2:}\label{it:update_step2} Solve
\begin{align} \label{eq:step2}
\det( {\cal M}_s ) \bm{y} = \bm{z},
\end{align}
then update per \eqref{eq:RK_update}, $\bm{u}_{n+1} = \bm{u}_n + \delta t \bm{y}$
\end{enumerate}


\subsection{On Step \ref{it:update_step1}}

The adjugate of ${\cal M}_s$ is a matrix defined over degree  $s-1$ polynomials in ${\cal L} \in \mathbb{R}^{m \times m}$. More specifically, we write it as
\begin{align}
\adj ({\cal M}_s) =
\begin{bmatrix}
Q_{11}({\cal L}) & \cdots & Q_{1s}({\cal L}) \\
\vdots & & \vdots \\
Q_{s1}({\cal L}) & \cdots & Q_{ss}({\cal L})
\end{bmatrix}
\in \mathbb{R}^{ms \times ms},
\quad
Q_{ij}({\cal L}) = \sum \limits_{k = 0}^{s-1} \hat{q}_{ijk} {\cal L}^k \in \mathbb{R}^{m \times m}.
\end{align}

Now, the Kronecker product appearing in front of this matrix simply takes inner products over its columns to give a block rectangular matrix whose elements are therefore polynomials in ${\cal L}$ of degree $s-1$:
\begin{align}
\big( \bm{b}_0^\top A^{-1}_0 \otimes I \big) \adj({\cal M}_s)
=
\begin{bmatrix}
X_{1}({\cal L}), \, \cdots, \, X_{s}({\cal L})
\end{bmatrix},
\end{align}
where
\begin{align}
X_{j}({\cal L}) = \sum \limits_{k = 0}^{s-1} \hat{x}_{j k} {\cal L}^k \in \mathbb{R}^{m \times m},
\quad
\hat{x}_{j k} = \sum \limits_{\ell = 1}^s \big( \tilde{\bm{b}}_0^\top \big)_{\ell} \hat{q}_{\ell j k}.
\end{align}
And, so, finally, the vector in \eqref{eq:step1} can be written as the sum
\begin{align} \label{eq:z_sum}
\bm{z} = \sum \limits_{i = 1}^s X_i({\cal L}) \bm{f}_i.
\end{align}
The main task here is going to be computing the action of the degree $s-1$ polynomials $X_i({\cal L})$ of the components of $\bm{f}$; note that we can  easily compute the sets of coefficients $\{ x_{jk} \}_{(j,k)=(1,0)}^{(s,s-1)}$. I think the most efficient way to compute the action of this polynomial is with a Horner-like scheme which is a well-known technique for evaluating scalar polynomials (see \url{https://en.wikipedia.org/wiki/Horner\%27s_method}). Basically, we can compute the action of the $n$th degree polynomial $P_n({\cal L})$ on a vector using: $n$ \texttt{MATVECs} with ${\cal L}$, $n+1$ \texttt{AXPYs} ($\bm{x} \gets \alpha \bm{y} + \beta \bm{z}$), $n$ \texttt{copies} ($n$ lots of copying values from one vector to another, $n \times [\bm{x} \gets \bm{y}]$), and one intermediate/temporary vector. So the main cost in computing \eqref{eq:z_sum} is $s(s-1)$ \texttt{MATVECs} with ${\cal L}$.
\end{comment}

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\section{Numerical results}\label{sec:numerics}

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{Finite-difference advection-diffusion}\label{sec:numerics:fd}

In this section, we consider a constant-coefficient advection-diffusion problem
discretized in space with high-order finite-differences. An exact solution to
this problem is used to demonstrate the high-order accuracy of the IRK methods,
and the robustness of the algorithms developed in the previous section with
respect to mesh resolution. Specifically, we solve the PDE
%
\begin{align}
\label{eq:FD_ex} u_t + 0.85 u_x + u_y = 0.3 u_{xx} + 0.25 u_{yy} + s(x,y,t),
\quad (x,y,t) \in (-1,1)^2 \times (0,2],
\end{align}
%
on a periodic spatial domain. The source term $s(x,y,t)$ is chosen such that
the solution of the PDE is
$u(x,y,t)=\sin^4(\pi/2[x-1-0.85t]) \sin^4(\pi/2 [y-1-t]) \exp(-[0.3+0.25]t)$.

We consider tests using IRK methods of orders three, four, seven, and eight. The
3rd- and 4th-order IRK methods are paired with 4th-order
central-finite-differences in space, and the 7th- and 8th-order methods with
8th-order central-finite-differences in space. In all cases, a time-step of
$\delta t = 2 h$ is used, with $h$ denoting the spatial mesh size. Due to the
diffusive, but non-SPD nature of the spatial discretization, we apply GMRES(30)
preconditioned by a classical algebraic multigrid (AMG) method in the
\textit{hypre} library \cite{Falgout:2002vu}. Specifically, we
use classical interpolation (type 0), Falgout coarsening (type 6) with a strength
tolerance $\theta_C = 0.25$, zero levels of aggressive coarsening, and
$L_1$-Gauss--Seidel relaxation (type 8), with an absolute and relative stopping
tolerance of $10^{-13}$. A single iteration of AMG is applied to approximate
$(\gamma_* I - \delta t {\cal L})^{-1}$.

In \Cref{fig:FD_ex}, discretization errors are shown for different IRK
methods, alongside the average number of AMG iterations needed per time step.
The expected asymptotic convergence rates (black dashed lines in the left panel)
are observed for all discretizations.\footnote{An exception here is A--SDIRK(4),
which appears to be converging with a rate closer to three than four; however,
further decreasing $\delta t$ (not shown here) confirms 4th-order convergence
is achieved eventually.}

\begin{figure}[!htb]
%\begin{figure}[H]
\centerline{
\includegraphics[width = 0.475\textwidth]{figures/FD_ex/errors_iters_14_34_23_-14_4_d2_ex1.pdf}
\quad
\includegraphics[width = 0.475\textwidth]{figures/FD_ex/amg_iters_14_34_23_-14_4_d2_ex1.pdf}
}
\centerline{
\includegraphics[width = 0.475\textwidth]{figures/FD_ex/errors_iters_18_38_27_d2_ex1.pdf}
\quad
\includegraphics[width = 0.475\textwidth]{figures/FD_ex/amg_iters_18_38_27_d2_ex1.pdf}
}
\caption{Finite-difference advection-diffusion problem \eqref{eq:FD_ex}. $L_{\infty}$-discretization errors at $t = 2$ as a function of time-step $\delta t$ are shown on
the left for various discretizations of approximately 4th order (top) and 8th order (bottom). Black, dashed lines with slopes of three and four are shown (top), as are those with slopes of seven and eight (bottom). Plots on the right show the average number of AMG iterations per time step.
\label{fig:FD_ex}
}
\end{figure}

The preconditioner appears robust with respect to mesh and problem size, since
the average number of AMG iterations per time step (which is a proxy for the
number of GMRES iterations) remains roughly constant as the the mesh is refined.
Of the fully implicit methods, the Gauss methods require the fewest AMG iterations, closely
followed by Radau IIA methods, with the Lobatto IIIC methods requiring the most
AMG iterations. This is
consistent with the theoretical estimates in \Cref{tab:beta,tab:cond}.
Note that while Gauss and Radau IIA methods have very similar
iteration counts, Gauss converges at one order faster, which can be seen in the
left-hand panel of the figure.

Considering the lower-order methods in the top
row of \Cref{fig:FD_ex}, L--SDIRK(4), a 5-stage, 4th-order, L-stable SDIRK
method requires the most AMG iterations of all methods. A--SDIRK(4), a 3-stage,
4th-order, A-stable SDIRK method, requires far fewer AMG iterations than
L--SDIRK(4). However, A--SDIRK(4) yields a significantly larger discretization
error than the other 4th-order schemes, and takes longer to reach its asymptotic
convergence rate. Thus, in terms of work done per accuracy, the new
preconditioner with 4th-order Gauss integration is the clear winner
for this particular test problem, requiring roughly half the AMG iterations
of the commonly used L-stable SDIRK4 scheme.

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{DG advection-diffusion}\label{sec:numerics:dg}

Here we consider a more difficult advection-diffusion problem, discretized
using high-order DG finite elements. In particular, we demonstrate
the effectiveness and scalability of the new preconditioning on more complex flows and
DG discretizations (\Cref{fig:ad_advdiff} and \Cref{sec:numerics:dg:adv}),
the reduction in computational work that can be achieved using large time steps and
very high-order integration (\Cref{sec:numerics:dg:adv}), and how
using multiple ``inner'' preconditioning iterations to approximate $\widehat{\mathcal{L}}^{-1}$
(or even inner Krylov acceleration) can be important for the fast, scalable
application of IRK integration (\Cref{sec:numerics:dg:diff}).

The governing equations in spatial domain $\Omega = [0,1] \times [0,1]$ are given by
\begin{equation} \label{eq:adv-diff}
	u_t + \nabla \cdot ( \bm\beta u  - \varepsilon \nabla u ) = f
\end{equation}
where $\bm\beta(x,y) : = (\cos(4\pi y), \sin(2 \pi x))^T$
is the prescribed velocity field and $\varepsilon$ the diffusion coefficient.
Periodic boundary conditions are enforced on $\partial\Omega$, and
\eqref{eq:adv-diff} is discretized with an upwind DG method \cite{Cockburn2001},
where diffusion terms are treated with the symmetric interior penalty method
\cite{Arnold1982,Arnold2002}. The resulting finite element problem is to find
$u_h \in V_h$ such that, for all $v_h \in V_h$,
\[
	\begin{multlined}
	\int_\Omega \partial_t (u_h) v_h \, dx
	- \int_\Omega u_h \bm\beta \cdot \nabla_h v_h \, dx
	+ \int_\Gamma \widehat{u_h} \bm\beta \cdot \llbracket v_h \rrbracket \, ds
	+ \int_\Omega \nabla_h u_h \cdot \nabla_h v_h \, d\bm x \\
	- \int_\Gamma \{ \nabla_h u_h \} \cdot \llbracket v_h \rrbracket \, ds
	- \int_\Gamma \{ \nabla_h v_h \} \cdot \llbracket u_h \rrbracket \, ds
	+ \int_\Gamma \sigma \llbracket u_h \rrbracket \cdot \llbracket v_h \rrbracket \, ds
	= \int_\Omega f v_h \, dx,
	\end{multlined}
\]
\OK{Is the $x$ on the top right really meant to be in bold?}
where $V_h$ is the DG finite element space consisting of piecewise polynomials of degree
$p$ defined on elements of the computational mesh $\mathcal{T}$ of the spatial domain
$\Omega$. No continuity is enforced between mesh elements.
Here, $\nabla_h$ is the broken gradient, $\Gamma$ denotes the skeleton of the mesh,
and $\{ \cdot \}$ and $\llbracket \cdot \rrbracket$ denote the average and jump of a
function across a mesh interface.
$\widehat{u_h}$ is used to denote the upwind numerical flux.
This discretization has been implemented in the MFEM finite element framework
\cite{Anderson2020}, and uses AMG preconditioning with approximate ideal restriction
(AIR) \cite{Manteuffel:2019,Manteuffel:2018} (after first scaling by the inverse of the
block-diagonal mass matrix).

The DG method is particularly well-suited for advection-dominated problems.
In the following subsections we vary $\varepsilon$ from $0$ (purely advective) to $0.01$.
The velocity field, initial condition, and numerical solution for $\varepsilon = 10^{-6}$
are shown in \Cref{fig:ad_advdiff}.

% srun -n32 ./dg_adv_diff  -rs 3 -rp 1 -i 18 -o 4 -dt 0.1 -air 2 -tf 10 -e 1e-6 -nov
% dt = 0.1, hmin = 0.015625, hmax = 0.015625
% time order = 8, space order = 4
% time acc. = 1e-08, space acc. = 5.96046e-08
%
\begin{figure}[!htb]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{./figures/velocity_field.pdf}
    \caption{Velocity field}\label{fig:v}
  \end{subfigure}
   \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{./figures/solution.0000.png}
    \caption{$t = 0$}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{./figures/solution.0003.png}
    \caption{$t = 0.3$}
  \end{subfigure}
  \\\vspace{2ex}
   \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{./figures/solution.0008.png}
    \caption{$t = 0.8$}
  \end{subfigure}
   \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{./figures/solution.0020.png}
    \caption{$t = 2.0$}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{./figures/solution.0070.png}
    \caption{$t = 7.0$}
  \end{subfigure}
  \\\vspace{2ex}
      \caption{DG advection-diffusion problem with velocity field shown in
      subplot (a) and the solution plotted for various time points from
      $t=0$ to $t = 7.0$ in subplots (b-f). Heatmap indicates solution in the
      2d domain, with blue $\mapsto 0$ and red $\mapsto 1$.}
  \label{fig:ad_advdiff}
\end{figure}

% ------------------------------------------------------------------------------------- %
\subsubsection{A hyperbolic example, and preconditioning with $\eta$ vs. $\gamma_*$}
\label{sec:numerics:dg:const}

First, we demonstrate the effectiveness of using $\gamma_*$ (\Cref{sec:solve:gamma}) 
\OK{What should this reference?}
instead of $\eta$ in the preconditioner, as well as the robustness of the proposed
method on a fully hyperbolic problem, where most papers have only discussed parabolic
PDEs. Thus, we set the diffusion coefficient $\varepsilon = 0$
and apply AIR as a preconditioner for individual systems
$(\gamma M - \delta t\mathcal{L})$.

AIR was originally designed for upwind DG discretizations of advection
and is well-suited for this problem. We use the \textit{hypre} implementation,
with distance 1.5 restriction with strength tolerance $\theta_R=0.01$, one-point
interpolation (type 100), Falgout coarsening (type 6) with strength tolerance
$\theta_C=0.1$, no pre-relaxation, and forward Gauss Seidel
post-relaxation (type 3), first on F-points followed by a second sweep on
all points. The domain is discretized using 4th-order finite elements on a
structured mesh, and the time step for each integration scheme is chosen
such that the spatial and temporal orders of accuracy match; for example,
for 8th-order integration we choose $\delta t = \sqrt{h}$, for mesh spacing
$h$, so that $\delta t^8 = h^4$. All linear systems are solved to a relative
tolerance of $10^{-12}$. There are a total of 1,638,400 spatial degrees-of-freedom
(DOFs), and the simulations are run on 288 processors on the Quartz machine at
Lawrence Livermore National Lab, resulting in $\sim$5600 DOFs/processor.

\Cref{tab:gamma} shows the average number of AIR iterations to solve for
each stage of an IRK method using $\eta$ and $\gamma_*$ as the preconditioning
constants. Iteration counts are shown for Gauss, Radau IIA, and Lobatto IIIC integration,
with 2--5 stages, and the (factor of) reduction in iteration count achieved using $\gamma_*$
vs. $\eta$ is also shown. For 5-stage Lobatto IIIC integration, $\gamma_*$ yields
almost a $6\times$ reduction in total inner AIR iterations to solve for the
``hard'' stage ($\beta > \eta$), while in no cases is there an increase in
iteration count when using $\gamma_*$.

{
% \renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{table}[!ht]
  \centering
  \begin{tabular}{|l || r | rr | rr | rrr |}  % chktex 44
  \hline
\multicolumn{9}{|c|}{Gauss}\\\hline
Stages/Order & 2/4 & \multicolumn{2}{c}{3/6} & \multicolumn{2}{|c}{4/8} & \multicolumn{3}{|c|}{5/10} \\\hline
Iterations$(\eta)$ & 17 & 6 & 30 & 11  & 47 & 8 & 16 & 70 \\
Iterations$(\gamma_*)$ & 11 & 6 & 15 & 10 & 19 & 8 & 13 & 23 \\\hline
Speedup & 1.5 & 1.0 & 2.0 & 1.1 & 2.5 & 1.0 & 1.2 & 3.0 \\\hline\hline
\multicolumn{9}{|c|}{Radau IIA}\\\hline
Stages/Order & 2/3 & \multicolumn{2}{c}{3/5} & \multicolumn{2}{|c}{4/7} & \multicolumn{3}{|c|}{5/9} \\\hline
Iterations$(\eta)$ & 12 & 5 & 39 & 11 & 64 & 8 & 16 & 97 \\
Iterations$(\gamma_*)$ & 12 & 5 & 18 & 9 & 21 & 8 & 12 & 25 \\\hline
Speedup & 1.0 & 1.0 & 2.2 & 1.2 & 3.0 & 1.0 & 1.3 & 3.9 \\\hline\hline
\multicolumn{9}{|c|}{Lobatto IIIC}\\\hline
Stages/Order & 2/2 & \multicolumn{2}{c}{3/4} & \multicolumn{2}{|c}{4/6} & \multicolumn{3}{|c|}{5/8} \\\hline
Iterations$(\eta)$ & 8 & 3 & 67 & 11 & 113 & 7 & 17 & 175 \\
Iterations$(\gamma_*)$ & 8 & 3 & 22 & 9 & 26 & 7 & 12 & 30 \\\hline
Speedup & 1.0 & 1.0 & 3.0 & 1.2 & 4.3 & 1.0 & 1.4 & 5.8 \\\hline\hline
  \end{tabular}
  \caption{Average AIR iterations to solve for each stage in an implicit
  Runge-Kutta method using preconditioners $(\eta M - \delta t \mathcal{L})^{-2}$ and
  $(\gamma_* M - \delta t \mathcal{L})^{-2}$ \eqref{eq:gam_opt}. 
  \OK{What should this reference?} 
  The ratio of
  iterations$(\eta)$/iterations$(\gamma_*)$ is shown in the ``Speedup'' rows.}
  \label{tab:gamma}
\end{table}
}


% ------------------------------------------------------------------------------------- %
\subsubsection{High-order \& advection-dominated}\label{sec:numerics:dg:adv}

This section demonstrates high-order IRK integration applied to the advection-dominated
problem in \Cref{fig:ad_advdiff}, using AIR as a preconditioner for the linear
systems.

\Cref{fig:dg_o4_abs} shows the total number of AIR iterations required to
compute one time step using six different Runge-Kutta schemes, from 4th
to 10th order, as a function of mesh spacing $h$. Note that as $h\to 0$,
there is only modest growth in the number of AIR iterations per time step,
indicating good (but not perfect) scalability.
Moreover, note that there is small growth in the number of iterations for
SDIRK4 as well (increasing from 20 to 25), which suggests the growth in
iteration count is due to imperfect scalability of AIR iterations rather
than the new IRK preconditioning.

\Cref{fig:dg_o4_rel} then shows the relative number of AIR iterations to
compute a given accuracy compared to SDIRK4. For example, if $h=0.01$,
then SDIRK4 uses $\delta t_4 = 0.01$ and Gauss8 uses $\delta t_8 =
\sqrt{0.01} = 0.1 = 10\delta t_4$, that is, $10\times$ less time steps to
achieve comparable accuracy. Note that as $h\to 0$, this factor becomes progressively
larger. For quite moderate $h$, we see how very high-order integration
can quickly lead to a reduction in total preconditioning iterations compared to a
standard SDIRK4 scheme. Gauss8 and Radau9, for example, require almost
$7\times$ less AIR iterations than SDIRK4 at $h =\delta t_4 = 1/256$,
and this ratio continues to decrease for smaller $\delta t$. Although this
does not account for additional setup time, where Gauss8 and Radau9 require
solving two and three different linear systems, respectively, and SDIRK4
only one, very high-order
integration permitted through the new preconditioning still offers the
opportunity for significant speedups.

%
\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
    \includegraphics[width=\textwidth]{./figures/dg_advdiff_o4_1e-6.pdf}
    \caption{Total AIR iterations per time step}
	\label{fig:dg_o4_abs}
  \end{subfigure}
   \begin{subfigure}[b]{0.475\textwidth}
    \includegraphics[width=\textwidth]{./figures/dg_advdiff_o4_1e-6_rel.pdf}
   \caption{AIR iterations relative to SDIRK4}
	\label{fig:dg_o4_rel}
  \end{subfigure}
\caption{}
  \label{fig:dg_o4}
\end{figure}
%


% ------------------------------------------------------------------------------------- %
\subsubsection{Diffusive problems and inner Krylov}\label{sec:numerics:dg:diff}

In \cite{Manteuffel:2019}, AIR was shown to be effective on some DG advection-diffusion
problems, and classical AMG is known to be effective on diffusion-dominated
problems. However, the region of comparable levels of advection and diffusion
remains the most difficult from a multigrid perspective. We use this to
demonstrate how methods developed here require a ``good'' preconditioner
for a backward Euler time step, $(\gamma M - \delta t \mathcal{L})^{-1}$,
in order to converge on more general IRK
methods. Fortunately, ensuring a preconditioner is sufficiently good can be
resolved by appealing to standard block preconditioning techniques, where an inner
iteration is used that applies multiple AIR iterations as a single preconditioner.

Here we consider an analogous problem to above, but set the diffusion coefficient
to $\varepsilon = 0.01$. We use a mesh with spacing $h \approx 0.001$, 2nd-order
DG finite elements, a time step of $\delta t = 0.1$, and three-stage 6th-order
Gauss integration. Altogether, this yields equal orders of accuracy, with time and
space error $\sim10^{-6}$. FGMRES \cite{saad1993flexible} is used for the
outer iteration, which allows for GMRES to be applied in an inner iteration
as a preconditioner for $(\gamma_* M - \delta t \mathcal{L})$. \Cref{fig:dg_o2} plots the
total number of AIR iterations per time step as a function of the number of
AIR iterations applied for each application of the preconditioner, using an
inner GMRES or an inner fixed-point. An advection-dominated problem
with $\varepsilon = 10^{-6}$ is also shown for comparison.

Recall we have three stages, one of which is a single linear system corresponding
to a real eigenvalue, and the other corresponding to a pair of complex conjugate
eigenvalues, which we precondition as in \Cref{sec:solve}. The latter ends up being
the more difficult problem to solve -- for $\varepsilon =0.01$ (\Cref{fig:dg_o2_1e-2}),
the outer FGMRES iteration for the complex conjugate quadratic does not converge in
1000 iterations when using one AIR iteration as a preconditioner.
If two AIR iterations with GMRES are used as a preconditioner,
the FGMRES iteration converges in approximately 130 iterations, each of which
requires two applications of GMRES preconditioned with two AIR iterations,
yielding just over 500 total AIR iterations to converge. Further increasing
the number of AIR iterations per preconditioning yields nice convergence
using inner fixed-point or GMRES, with 150 and 112 total AIR iterations per
time step, respectively. In contrast, \Cref{fig:dg_o2_1e-6} shows that
additional AIR iterations for the advection-dominated case are generally
detrimental to overall computational cost (although the outer iteration
converges slightly faster, it does not make up for the additional linear
solves/iteration).

\begin{figure}[h!]
\centering

  \centering
  \begin{subfigure}[b]{0.475\textwidth}
	\includegraphics[width = \textwidth]{./figures/dg_advdiff_o2_1e-6.pdf}
	\caption{$\varepsilon = 10^{-6}$.}
	\label{fig:dg_o2_1e-6}
  \end{subfigure}
   \begin{subfigure}[b]{0.475\textwidth}
	\includegraphics[width = \textwidth]{./figures/dg_advdiff_o2_1e-2.pdf}
	\caption{$\varepsilon = 0.01$.}
	\label{fig:dg_o2_1e-2}
  \end{subfigure}
\caption{AIR iterations per time step as a function of the number of
AIR iterations applied in each application of the preconditioner, for
diffusion coefficient $\varepsilon$.}
\label{fig:dg_o2}
\end{figure}


% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\subsection{High-order matrix-free diffusion}

In this example, we illustrate the use of high-order IRK methods coupled with high-order finite element spatial discretizations.
It is well-known that matrix assembly becomes prohibitively expensive for high-order finite elements.
Naive algorithms typically require $\mathcal{O}(p^{3d})$ operations to assemble the resulting system matrix, where $p$ is the polynomial degree and $d$ is the spatial dimension.
Techniques such as sum factorization can reduce this cost on tensor-product elements to $\mathcal{O}(p^{2d+1})$, however this cost can still be prohibitive for large values of $p$ \cite{Melenk2001}.
On the other hand, matrix-free operator evaluation on tensor-product meshes can be performed in $\mathcal{O}(p^{d+1})$ operations \cite{Orszag1980}, motivating the development of solvers and preconditioners that can be constructed and applied without access to the assembled system matrix \cite{Kronbichler2019}.

We consider a high-order finite element discretization of the linear heat equation on spatial domain $\Omega$,
\[
	\int_\Omega \partial_t (u_h) v_h \, dx + \int_\Omega \nabla u_h \cdot \nabla v_h \, dx = \int_\Omega f v_h \, dx,
\]
where $u_h, v_h \in V_h$, and $V_h$ denotes the degree-$p$ $H^1$-conforming finite element space defined on a mesh $\mathcal{T}$ consisting of tensor-product elements (i.e.\ quadrilaterals or hexahedra).
The matrix-free action of the corresponding operator is computed in $\mathcal{O}(p^{d+1})$ operations using the \textit{partial assembly} features of the MFEM finite element library \cite{Anderson2020}.
In order to precondition the resulting system, we make use of a low-order refined preconditioner, whereby the high-order system is preconditioned using a spectrally equivalent low-order finite element discretization computed on a refined mesh \cite{Canuto2010}.
The low-order refined discretization can be assembled in $\mathcal{O}(p^d)$ time, thereby avoiding the prohibitive costs of high-order matrix assembly.
We make use of the uniform preconditioners for the low-order refined problem based on subspace corrections, developed in \cite{Pazner2019a}.

For this test case, take the spatial domain to be $\Omega = [0,1] \times [0,1]$, with periodic boundary conditions.
We choose the forcing term
\[
	f(x, y, t) = \sin (2\pi x)\cos(2\pi y) \left(\cos(t) + 8 \pi^2 (2 + \sin(t)) \right),
\]
which corresponds to the exact solution
\[
	u(x, y, t) = \sin(2\pi x)\cos(2\pi y)(2 + \sin(t)).
\]
We begin with a very coarse $3 \times 3$ mesh, and integrate in time until $t=0.1$ using the Gauss and Radau IIA methods of orders 2 through 10.
For each test case, the finite element polynomial degree is set to $k-1$, where $k$ is the order of accuracy of the time integration method, resulting in $k$th order convergence in both space and time.
The mesh and time step are refined by factors of two to confirm the high-order convergence in space and time of the method.
The relative $L^2$ error, obtained by comparing against the exact solution, is shown in \Cref{fig:high-order-diff-errors}.

\begin{figure}[!ht]
	\centering
	\includegraphics{figures/high_order_diff_error_plot/ho_diff_errors}
	\caption{
		High-order convergence in space and time for the matrix-free diffusion problem.
		Gauss and Radau IIA methods of orders 2 through 10 are used.
		The dashed lines indicate the expected rates of convergence for each method.
	}
	\label{fig:high-order-diff-errors}
\end{figure}

We also use this test case to study the effect of inner iterations on the convergence of the iteration solver.
As discussed in \Cref{sec:inexact-precond}, it is important that the underlying preconditioner provide a good approximation of the inverse of the operator.
For that reason, we consider the use of an inner Krylov solver at every iteration.
Since this corresponds to using a variable preconditioner at each iteration, a flexible Krylov method may have to be used for the outer iteration, although in practice good convergence is often still observed using the standard CG method \cite{Notay2000}.
In particular, we compare the total number of preconditioner applications required to converge the outer iteration to a relative tolerance of $10^{-10}$, both with and without an inner Krylov solver.
For the inner Krylov solver, we use a CG iteration with the same relative tolerance as the outer iteration in order to give a good approximation to the inverse of the operator.
The iteration counts are displayed in \Cref{fig:high-order-diff-iters}.
We note that for the fully implicit IRK methods, using an inner Krylov solver can reduce the total number of preconditioner applications by about a factor of 1.5, although this depends on the type of method and order of accuracy.
As expected, the use of inner iterations does not reduce the total number of preconditioner applications for DIRK methods.
In addition, for this test case, the total number of preconditioner applications required for the second and fourth order Gauss IRK methods is between 1.3 and 2 times smaller than those required for the corresponding equal-order DIRK methods.

\begin{figure}[!ht]
	\centering
	\includegraphics{figures/high_order_diff_iters/high_order_diff_iters}
	\caption{
		Comparison of total number of preconditioner applications with and without
		inner iterations.
		Both the outer iteration and the inner CG iteration are converged to a
		relative tolerance of $10^{-10}$.
	}
	\label{fig:high-order-diff-iters}
\end{figure}

% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------- %
\section{Conclusions}\label{sec:conc}

This paper introduces a theoretical and algorithmic framework for the fast, parallel
solution of fully implicit Runge-Kutta and DG discretizations in time for linear
numerical PDEs. Theory is developed to guarantee $\mathcal{O}(1)$ conditioning under
general assumptions on the spatial discretization that yield stable time integration.
Numerical results demonstrate the new method on various high-order finite-difference
and finite-element discretizations of linear parabolic and hyperbolic problems,
demonstrating fast, scalable solution of up to 10th order accuracy. In several
cases, the new method can achieve 4th-order accuracy using Gauss integration
with roughly half the number of preconditioner applications as required using
standard SDIRK techniques. Ongoing work involves addressing fully nonlinear
problems and algebraic constraints, in particular, without assuming that the
linear system \eqref{eq:k0} can be expressed in Kronecker-product form (thus
allowing for a true Newton or better Newton-like method compared with the
commonly used/analyzed simplified Newton approach).

% ------------------------------------------------------------------------------- %
\bibliographystyle{siamplain}
\bibliography{refs2.bib}


\end{document}
