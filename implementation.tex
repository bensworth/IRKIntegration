% \documentclass[a4paper,10pt]{article}
\documentclass[review]{siamart}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{empheq}
\usepackage{enumitem}
	\setlist{nosep} % or \setlist{noitemsep} to leave space around whole list
\usepackage{color}
\usepackage{showlabels}
\usepackage{adjustbox}
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}
\usepackage[numbers,sort]{natbib}
\usepackage{cleveref}

\newsiamremark{remark}{Remark}


% \newtheorem{lemma}{Lemma}
% \newtheorem{definition}{Definition}
% \newtheorem{theorem}{Theorem}
% \newtheorem{corollary}{Corollary}

\newcommand{\tcb}{\textcolor{blue}}
\newcommand{\tcp}{\textcolor{purple}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO\@: #1]}}

\newcommand{\mdet}{\operatorname{det}}
\newcommand{\madj}{\operatorname{adj}}

% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %

\newcommand{\TheTitle}{IRK!}
\newcommand{\TheAuthors}{B.S. Southworth }
\headers{IRK!}{\TheAuthors}
\title{{\TheTitle}\thanks{This research was conducted ...
  }}

\author{%
  Ben~S.~Southworth
  \thanks{Department of Applied Mathematics,
          University of Colorado at Boulder
          (\email{ben.s.southworth@gmail.com}).}
}

\ifpdf%
\hypersetup{%
  pdftitle={\TheTitle},
  pdfauthor={\TheAuthors}
}
\fi

% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %

\begin{document}
\allowdisplaybreaks

% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %
\section{MFEM}\text{ }

\noindent
IRK : public ODESolver
\begin{itemize}
	\item Must implement virtual void functions:
	\begin{itemize}
		\item \textit{Step(Vector \&x, double \&t, double \&dt)=0} -- Perform a time
		step from time $t$ to time $t+dt$ based on the requested step size $dt$.
		\item \textit{Run(Vector \&x, double \&t, double \&dt, double tf)} -- Perform time integration from time $t$ to time $tf$.
	\end{itemize}

	\item This class should contain all the underlying algorithm, including
	CharPolyPrecon and CharPolyOp. The latter classes should call Mult() from
	the user-defined solver in IRKSpatialDisc. 
\end{itemize}

\noindent
IRKSpatialDisc : public TimeDependentOperator
\begin{itemize}
	\item Must implement virtual void functions:
	\begin{itemize}
		\item \textit{Mult(const Vector \&x, Vector \&y) const}
		\item \textit{ImplicitSolve(const double dt, const Vector \&x, Vector \&k)}.
	\end{itemize}

	\item This will be an abstract base class that a user will implement a derived
	version of, providing solvers for $(M - c\delta t \mathcal{L})$ and the application
	of $M^{-1}\mathcal{L}$. I believe these are ImplicitSolve and Mult, repsectively
	(check on Mult).

	\item It is possible this class could just be a TimeDependentOperator, but we
	can start with its own more specific class and switch to TimeDependentOperator
	later if it makes sense.

\end{itemize}



% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %
\section{Implementation details:  The time-independent case}

The basic idea is that at each time step, we need to update the previous solution with
\begin{align} \label{eq:RK_update}
\bm{u}_{n+1}  = \bm{u}_n + \delta t \mdet({\cal M}_s)^{-1} \big( \bm{d}^\top_0 \otimes I \big) \madj({\cal M}_s) \big( I \otimes M^{-1} \big) \bm{f},
\end{align}
where we have
\begin{itemize}
\item $m \in \mathbb{N}$: The dimension of the spatial problem

\item $\bm{f} = (\bm{f}_1, \ldots, \bm{f}_s)^\top$, with $\bm{f}_j = {\cal L} \bm{u}_n + \bm{g}(t_n + c_j \delta t) \in \mathbb{R}^m$

\item $\bm{d}_0 \coloneqq A^{-\top}_0 \bm{b}_0 \in \mathbb{R}^{s}$ 

\item $M \in \mathbb{R}^{m \times m}$ is the mass matrix
\end{itemize}

Now we break \eqref{eq:RK_update} into two steps:
\begin{enumerate}
\item{Step 1:}\label{it:update_step1} Compute 
\begin{align} \label{eq:step1}
\bm{z} = \big( \bm{d}^\top_0 \otimes I \big) \madj ({\cal M}_s) \big(I \otimes M^{-1} \big)\bm{f} \in \mathbb{R}^m
\end{align}

\item{Step 2:}\label{it:update_step2} Solve
\begin{align} \label{eq:step2}
\mdet( {\cal M}_s ) \bm{y} = \bm{z},
\end{align}
then update per \eqref{eq:RK_update}, $\bm{u}_{n+1} = \bm{u}_n + \delta t \bm{y}$
\end{enumerate}

Note that problem arises from solving the linear system for $(A_0 \otimes I_m) \bm{k}$
\begin{align}
{\cal M}_s (A_0 \otimes I_m) \bm{k} = (I_s \otimes M^{-1}) \bm{f}, 
\quad 
{\cal M}_s \coloneqq A_0^{-1} \otimes I_m - I_s \otimes \delta t M^{-1} {\cal L}.
\end{align}


% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ 
\subsection{On Step \ref{it:update_step1}}

$\madj ({\cal M}_s)$ is defined by evaluating the adjugate of $A_0^{-1} - zI$ at $z = \delta t \widehat{\cal L}$, where $\tcb{\widehat{\cal L} \coloneqq M^{-1} {\cal L}}$, which is a matrix over polynomials of degree $s-1$ in $z$. If we write such polynomials as $Q_{ij}(z) \coloneqq \madj(A_0^{-1} - zI)_{ij}$, then
\begin{align}
\madj ({\cal M}_s) = 
\begin{bmatrix}
Q_{11}(\delta t \widehat{\cal L}) & \cdots & Q_{1s}(\delta t \widehat{\cal L}) \\
\vdots & & \vdots \\
Q_{s1}(\delta t \widehat{\cal L}) & \cdots & Q_{ss}(\delta t \widehat{\cal L})
\end{bmatrix}
\in \mathbb{R}^{ms \times ms},
\quad
Q_{ij}({\cal L}) = \sum \limits_{k = 0}^{s-1} \hat{q}_{ijk} {\cal L}^k \in \mathbb{R}^{m \times m}.
\end{align}
For sensible $s \in \mathbb{N}$, we can symbolically compute $\madj(A_0^{-1} - zI)$ in terms of an arbitrary matrix $A_0^{-1}$, and, thus, we can compute the sets of coefficients $\{ \hat{q}_{ijk} \}_{(i,j,k)=(1,1,0)}^{(s,s,s-1)}$.

Now, the rectangular Kronecker product matrix appearing in front of this matrix simply takes inner products over its columns to give a block rectangular matrix whose elements are therefore polynomials in $\widehat{\cal L}$ of degree $s-1$:
\begin{align}
\big( \bm{d}_0^\top \otimes I \big) \madj({\cal M}_s) 
=
\begin{bmatrix}
X_{1}(\delta t \widehat{\cal L}), \, \cdots, \, X_{s}(\delta t \widehat{\cal L})
\end{bmatrix},
\end{align}
where
\begin{align}
X_{j}({\cal L}) = \sum \limits_{k = 0}^{s-1} \hat{x}_{j k} {\cal L}^k \in \mathbb{R}^{m \times m}, 
\quad
\hat{x}_{j k} = \sum \limits_{\ell = 1}^s d_{0,\ell} \, \hat{q}_{\ell j k}.
\end{align}
Using the coefficients $\{ \hat{q}_{ijk} \}_{(i,j,k)=(1,1,0)}^{(s,s,s-1)}$ we can compute the coefficients $\{ \hat{x}_{jk} \}_{(j,k)=(1,0)}^{(s,s-1)}$, which are the things we actually need (that is, we don't need the intermediate $\hat{q}_{ijk}$). Thus, in the implementation, we have coefficients $\{ \hat{x}_{jk} \}_{(j,k)=(1,0)}^{(s,s-1)}$ implemented symbolically in terms of general $\bm{d}_0$ and $A_0^{-1}$ for sensible values of $s$. When the user specifies a particular Butcher tableaux (and hence a specific $s$), the appropriate set of coefficients are evaluated numerically and stored in memory so that the polynomials $\{X_{j}(\delta t \widehat{\cal L})\}$, can be evaluated as needed by the algorithm.

Finally, the vector in \eqref{eq:step1} can be written as the sum
\begin{align} \label{eq:z_sum}
\bm{z} = \sum \limits_{i = 1}^s [X_i(\delta t \widehat{\cal L})] (M^{-1} \bm{f}_i).
\end{align}
The main task here is computing the action of the degree $s-1$ polynomials $\{X_j(\delta t \widehat{\cal L})\}$ on the components $M^{-1} \bm{f}_i$.  \tcp{Ben: It's not true in general that $M^{-1}$ will commute with a monomial in $M^{-1} {\cal L}$, is it? So we have to do this mass matrix solve in the eqn. above on every component of $\bm{f}$ rather than pulling this $M^{-1}$ out the front of the entire sum?} \tcb{Yes. But if $\bm{f}_j = {\cal L} \bm{u}_n + \bm{g}(t_n + c_j \delta t)$,
applying $M^{-1}$ to this equation is the same as the Mult() function in MFEM I believe, which the user has to
implement in time dependent operator. If the forcing function $\mathbf{g}$ is indepenent of time, we could just
compute once, although it's really not that much work to do it once for each stage.}
I think the most efficient way to compute the action of such polynomials is with a Horner-like scheme, which is a well-known technique for evaluating scalar polynomials (see \url{https://en.wikipedia.org/wiki/Horner\%27s_method}). Basically, we can compute the action of the $n$th degree polynomial $P_n({\cal L})$ on a vector using: $n$ \texttt{MATVECs} with ${\cal L}$, $n+1$ \texttt{AXPYs} ($\bm{x} \gets \alpha \bm{y} + \beta \bm{z}$), $n$ \texttt{copies} ($n$ lots of copying values from one vector to another, $n \times [\bm{x} \gets \bm{y}]$), and one intermediate/temporary vector. Thus, the main cost in computing \eqref{eq:z_sum} is $s(s-1)$ \texttt{MATVECs} with $\widehat{\cal L} = M^{-1} {\cal L}$, and a secondary cost is the $s$ actions of $M^{-1}$ on the vectors $\{\bm{f}_j\}_{j = 1}^s$.



% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ 
\subsection{On Step \ref{it:update_step2}}
As shown previously, 
\begin{align}
\mdet ({\cal M}_s) = P_s(\delta t \widehat{\cal L}) = \prod \limits_{j = 1}^s (\lambda_j I - \delta t \widehat{\cal L}),
\end{align}
where $\{ \lambda_j \}_{j = 1}^s$ are the eigenvalues of $A_0^{-1}$. Now we're going to split the eigenvalues into two disjoint subsets based on their being real and complex:
\begin{align}
\{\lambda_j \}_{j = 1}^s = \{\zeta_j \}_{j = 1}^{s_{\Re}} \cup \{ \eta_j + \imath \beta_j, \eta_j - \imath \beta_j \}_{j = 1}^{s_{\Im}}, 
\quad \eta_j, \beta_j, \zeta_j \in \mathbb{R},
\quad s = s_{\Re} + 2 s_{\Im}.
\end{align}
That is, $\{\zeta_j \}_{j = 1}^{s_{\Re}}$ are the $0 \leq n_{\Re} \leq s$ real eigenvalues of $A_0^{-1}$, and $\{ \eta_{j} + \imath \beta_{j}, \eta_{j} - \imath \beta_{j} \}_{j = 1}^{s_{\Im}}$ are the $0 \leq {n_{\Im}} \leq \left \lfloor{s/2}\right \rfloor $ complex, conjugate-pair, eigenvalues of $A_0^{-1}$. 

To invert characteristic polynomial $P_s$, we do so by inverting only real factors. This means that complex-conjugate pairs are combined to become quadratic polynomials in ${\cal L}$. That is, we have
\begin{align} 
P_s (\delta t \widehat{\cal L}) 
&= 
\prod \limits_{j = 1}^{s_{\Re}} {\cal R}_j(\delta t \widehat{\cal L}) \prod \limits_{j = 1}^{s_{\Im}} {\cal I}_j(\delta t \widehat{\cal L}),\\
\quad 
{\cal R}_j(\delta t \widehat{\cal L}) 
&\coloneqq 
\zeta_j I - \delta t \widehat{\cal L}, \\
{\cal I}_j(\delta t \widehat{\cal L})
&\coloneqq
[\eta_j^2 + \beta_j^2] I - 2 \eta_j \delta t \widehat{\cal L} + (\delta t \widehat{\cal L})^2.
\end{align}
\begin{enumerate}
\item \textbf{Linear factors:} To solve the system 
\begin{align}
{\cal R}_j(\delta t \widehat{\cal L}) \bm{y}_j = (\zeta_j I -  \delta t M^{-1} {\cal L}) \bm{y}_j = \bm{z}_j,
\end{align}
we first scale both sides of the problem by ${M}$ to give
\begin{align}
(\zeta_j M -  \delta t {\cal L}) \bm{y}_j = M \bm{z}_j.
\end{align}
This system is then solved by GMRES (or CG/MINRES if ${\cal L}$ is symmetric) which is preconditioned by a single iteration of AMG applied to the matrix $(\zeta_j M -  \delta t {\cal L})$. So, per Krylov iteration, the action of $M$ is required once. Note that not scaling the system and applying an AMG preconditioner to the sparse matrix $(\zeta_j M -  \delta t {\cal L})$ requires the action of both $M$ and $M^{-1}$ every Krylov iteration.

\item \textbf{Quadratic factors:} To solve the system
\begin{align}
{\cal I}_j(\delta t \widehat{\cal L}) \bm{y}_j 
=
\big( [\eta_j^2 + \beta_j^2] I - 2 \eta_j \delta t M^{-1} {\cal L} + (\delta t M^{-1} {\cal L})^2 \big) \bm{y}_j
= \bm{z}_j,
\end{align}
we first scale both sides by $M$ to give
\begin{align}
\big( [\eta_j^2 + \beta_j^2] M - 2 \eta_j \delta t {\cal L} + \delta t^2 {\cal L} M^{-1} {\cal L} \big) \bm{y}_j
= M \bm{z}_j.
\end{align}
This system is then solved by GMRES (or CG/MINRES if ${\cal L}$ is symmetric; note the operator is symmetric in this instance \tcb{(but is it SPD?)}) which is preconditioned by \underline{twice} applying a \textit{small}, fixed number of iterations (likely one) of AMG to the matrix $(\eta_j M -  \delta t {\cal L})$; however, before the second application of AMG, the RHS needs to be scaled by $M$.\footnote{To see why this is true, say $\beta_j = 0$, then the system we'd be solving is $(\eta_j M - \delta t {\cal L}) (\eta_j I - \delta t M^{-1} {\cal L}) \bm{y}_j = M \bm{z}_j$, and, so, when we come to precondition this system by twice inverting $\eta_j M - \delta t L$, we can invert it once to solve the system $(\eta_j M - \delta t L) \hat{\bm{y}}_j =  M \bm{z}_j$, where $\hat{\bm{y}}_j = (\eta_j I - \delta t M^{-1} {\cal L}) \bm{y}_j$. But before we can solve $(\eta_j I - \delta t M^{-1} L) \bm{y}_j = \hat{\bm{y}}_j$ by inverting $(\eta_j M - \delta t L)$, we must first scale the system by $M$.} So, per Krylov iteration, the action of $M^{-1}$ is required once, and the action of $M$ twice. Note that if the system were not scaled originally by $M$, then the action of $M$ and $M^{-1}$ would be required twice each per Krylov iteration ($M^{-1}$ in the action of the operator, and $M$ to scale the right hand sides in the preconditioner).
\end{enumerate}


\tcp{Ben: So why cant we approximately invert $(\eta_j M - \delta t {\cal L})$ with GMRES preconditioned by AMG? You mentioned in the code I'd written that this doesn't work. My thinking was that we need to approximately invert $(\eta_j M - \delta t {\cal L})$ and GMRES preconditioned with AMG will do this more robustly than standalone AMG?}
{\color{blue}
GMRES forms an optimal linear combination over a Krylov space. The $k$th Krylov space takes the form of powers
of the preconditioned operator times an initial residual, $\mathcal{K} := \{\mathbf{r}_0, (M^{-1}A)\mathbf{r}_0, 
..., (M^{-1}A)^k\mathbf{r}_0\}$. This results in an approximation to the linear system given by a polynomial
in the preconditioned operator. For this to make sense, it is important that every time you take a further
GMRES iteration, which computes a new power, $(M^{-1}A)^k\mathbf{r}_0$, that you use the same $M^{-1}$ as in
previous iterations. Now suppose $M^{-1}$ actually corresponds to an inner GMRES approximation. Then every
outer GMRES iteration has a different right-hand side, and the result is that the inner GMRES approximate 
inverse is a different polynomial for every outer iteration. This means you are using a different preconditioner
for every iteration, and GMRES will be very unhappy.

Flexible GMRES is a GMRES-like algorithm that allows for different preconditioners each iteration. This would
work with what you are talking about, but also doesn't make sense here. Because full memory GMRES is already
optimal, it doesn't make sense to try to optimize the inner solve. You're always going to be taking a polynomial
in the preconditioned operator. If you use only outer GMRES, then every application of the AMG preconditioner is 
made to be optimal with respect to the larger quadratic linear system. If you use inner GMRES for, say, 10
iterations,  those 10 AMG applications are made optimal to approximate the inverse of the inner linear system,
but this is probably not optimal to solve the larger quadratic operator. Often in situations like this, you
actually gain very little improvement in convergence by applying more iterations to an inner linear system.
}


% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ 
\newpage
\section{Numerical results}

We numerically approximate the solution of the advection problem,
\begin{align}
u_t  + \nabla \cdot (\bm{\alpha}(\bm{x}) u) = s(\bm{x},t), \quad  \bm{x} \in (-1, 1)^d, \quad t \in (0, t_{\rm f}].
\end{align}
Some remarks:
\begin{itemize}
\item Periodic spatial boundaries

\item $t_{\rm f} \approx 2$

\item A $p$th-order IRK scheme is coupled with $p$th-order upwind finite differences in space.

\item A CFL number of $c \approx 2$ is used (not practical in terms of cost of solves per accuracy obtained, but good for testing implementation since don't have to worry about solvers not coping well with difficult systems)

\item Tests indicate that the code is implemented and working properly since theoretically predicated convergence rates are obtained in all cases.
\end{itemize}

\textbf{One dimension:} See Figure \ref{fig:errors1D}. Choose $s(x,t)$ such that
\begin{align}
\alpha(x) &= \tfrac{1}{2} \big[1 + \cos^2(\pi x) \big],
\\
u(x,t) &= \cos(\pi(x - t)) \exp(\cos(2\pi t) - 1).
\end{align}

\textbf{Two dimensions:} See Figure \ref{fig:errors2D}. Choose $s(x,y,t)$ such that
\begin{align}
\alpha(x,y) &= \left(
\tfrac{1}{4} \big[1 + \cos^2(\pi x) \big] \big[ 1 + \sin^2(\pi y) \big],
\tfrac{1}{4} \big[1 + \sin^2(\pi x) \big] \big[1 + \cos^2(\pi y) \big] 
\right), 
\\
u(x,y,t) &= \cos(\pi(x - t)) \cos(\pi(y - t)) \exp(\cos(2\pi t) - 1).
\end{align}


\begin{figure}[H]
\centerline{
\includegraphics[width = 0.55\textwidth]{figures/SDIRK_d1_FD4}
\quad
\includegraphics[width = 0.55\textwidth]{figures/Gauss_d1_FD4}
}
\centerline{
\includegraphics[width = 0.55\textwidth]{figures/RadauIIA_d1_FD4}
\quad
\includegraphics[width = 0.55\textwidth]{figures/LobattoIIIC_d1_FD4}
}
\caption{\textbf{One dimension:} $L_{\infty}$ errors measured at $t \approx 2$. Theoretically predicted convergence rates of ${\cal O}(p)$ are shown as dashed black lines.
\label{fig:errors1D}
}
\end{figure}

\begin{figure}[H]
\centerline{
\includegraphics[width = 0.55\textwidth]{figures/SDIRK_d2_FD4}
\quad
\includegraphics[width = 0.55\textwidth]{figures/Gauss_d2_FD4}
}
\centerline{
\includegraphics[width = 0.55\textwidth]{figures/RadauIIA_d2_FD4}
\quad
\includegraphics[width = 0.55\textwidth]{figures/LobattoIIIC_d2_FD4}
}
\caption{\textbf{Two dimensions:} $L_{\infty}$ errors measured at $t \approx 2$. Theoretically predicted convergence rates of ${\cal O}(p)$ are shown as dashed black lines. \tcp{Note SDIRK and Radau IIA are a bit messy... Not so sure why. Convergence seemed a bit better in $L_1$ and $L_2$. Note in particular that SDIRK3 is quite large at the coarsest resolution (the numerical solution grows, not sure why... Doesn't happen for constant coefficient... Note it's the same spatial disc. that causes issues for Radau IIA(3))}
\label{fig:errors2D}
}
\end{figure}


% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ 
\newpage
\newpage
\section{Numerical results II: Scalability of solvers}

\begin{itemize}
\setlength\itemsep{0.5em}

\item Kind of hard to decide what  to look at, there are many different parameters...

\item Solve 1D in space, variable-coefficient advection problem. \todo{Conduct same study for diffusion problem...}

\item Both absolute and relative GMRES tolerances are set to $1e-15$ for all problems (realistically, we should tighten as we refine the mesh to reach discretization error)

\item Pair $p$th-order IRK with  $p$th-order FD in space

\item Look at 4th-order examples in Figure \ref{fig:4th-order_solves}.
\begin{enumerate}
\item SDIRK4: Need to invert 5 linear factors (i.e., $\beta/\eta=0$ for all). Each GMRES iteration is preconditioned by one AMG iteration. 

\item Gauss4: Need to invert 1 quadratic factor ($\beta/\eta=0.58$). Each GMRES iteration is preconditioned by two applications of AMG. 

\item Lobatto4: Need to invert 1 quadratic factor ($\beta/\eta=1.49$), and one linear factor ($\beta/\eta=0$). Again, for the quadratic factor, each GMRES iteration is preconditioned by two applications of AMG. 

\end{enumerate}

\item Assuming the AMG preconditioners for each scheme have similar cycle complexities, per discretization error, the Gauss4 solve is cheaper than SDIRK4. The total work per time step for SDIRK4 is 5 GMRES solves (recalling there are 5 stages), and each of these GMRES solves is roughly half the cost of the single quadratic GMRES solve required by Gauss4 per time step.

\item Lobatto4 seems like it's not at all competitive in terms of cost due to the solution of its quadratic factor requiring so many iterations (kind of expected, since we know the quality of the preconditioner degrades with increasing $\beta/\eta$).

\end{itemize}

\begin{figure}[H]
\centerline{
\includegraphics[width = 0.5\textwidth]{figures/solver_convergence/SDIRK4_convergence}
\quad
\includegraphics[width = 0.5\textwidth]{figures/solver_convergence/SDIRK4_gmres}
}
\centerline{
\includegraphics[width = 0.5\textwidth]{figures/solver_convergence/Gauss4_convergence}
\quad
\includegraphics[width = 0.5\textwidth]{figures/solver_convergence/Gauss4_gmres}
}
\centerline{
\includegraphics[width = 0.5\textwidth]{figures/solver_convergence/Lobatto4_convergence}
\quad
\includegraphics[width = 0.5\textwidth]{figures/solver_convergence/Lobatto4_gmres}
}
\caption{Convergence of discrete solution \textbf{(left)}, and average number of GMRES iterations per time step for each linear system solved \textbf{(right)} as a function of $\delta t$ and CFL $\sim \delta t / \delta x$; note that increasing $\delta t/\delta x$ corresponds to decreasing $\delta x$ and increasing the size of the spatial problem. \textbf{Top:} SDIRK4+U4 (iterations may vary slighlty for the 5 linear systems due to them having different RHS), \textbf{Middle:} Gauss4+U4, \textbf{Bottom:} Lobatto4+U4.
\label{fig:4th-order_solves}
}
\end{figure}



% ------------------------------------------------------------------------------- %
% \bibliographystyle{siamplain}
% \bibliography{refs.bib}


% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------ 
\newpage
\newpage
\section{Nonlinear}
Have ODEs,
%
\begin{align}
	M\mathbf{u}'(t) =  \mathcal{N}(\mathbf{u},t) \quad\text{in }(0,T], \quad \mathbf{u}(0) = \mathbf{u}_0,
\end{align}
%
where $M$ is a mass matrix and $\mathcal{N} \colon \mathbb{R}^{N} \times \mathbb{R}_+ \to \mathbb{R}^{N}$ is a discrete, time-dependent, nonlinear operator. Then, an $s$-stage IRK scheme approximately propagates the discrete  solution by
%
\begin{align}
\mathbf{u}_{n+1} & = \mathbf{u}_n + \delta t \sum_{i=1}^s b_i\mathbf{k}_i,
\end{align}
with stage vectors satisfying the block system of $s$ nonlinear equations,
\begin{align}
M\mathbf{k}_i & = \mathcal{N}\left(\mathbf{u}_n + \delta t\sum_{j=1}^s a_{ij}\mathbf{k}_j, t_n+\delta tc_i\right), \quad i = (1,\ldots,s).
\end{align}

For a given pair $(\bm{u}_n,t_n)$, let's define the operator $F_i \colon \mathbb{R}^{sN} \to \mathbb{R}^N$ that encodes the $i$th stage vector,
\begin{align}
F_i(\bm{k}) \coloneqq M \bm{k}_i - {\cal N} \left(\bm{u}_n + \delta t \sum \limits_{j = 1}^s a_{ij} \bm{k}_j, t_n + c_i  \delta t \right),
\end{align}
with $\bm{k} = (\bm{k}_1, \ldots, \bm{k}_s)^\top$ the vector of stages. Then, the nonlinear system we need to solve to advance forwards one time step is
\begin{align}
\begin{bmatrix}
F_{1}(\bm{k})\\
\vdots \\
F_{s}(\bm{k})
\end{bmatrix}
\eqqcolon F(\bm{k}) = 0.
\end{align}

Let's say we solve this system via a Newton-type method. That is, we apply a fixed-point algorithm like 
\begin{itemize}
\item Given initial iterate $\bm{\hat{k}} \approx \bm{k}$, if $\Vert F(\bm{\hat{k}}) \Vert > \varepsilon$, 
\item Solve $G(\bm{\hat{k}}) \bm{d} = - F(\bm{\hat{k}})$, where $G(\bm{x}) \approx F'(\bm{x})$ is some approximation of the Jacobian of $F$ evaluated at $\bm{x}$
\item Update iterate $\bm{\hat{k}} \to \bm{\hat{k}} + \bm{d}$ 
\item If $\Vert F(\bm{\hat{k}}) \Vert > \varepsilon$, repeat
\end{itemize}

The particular method will be defined by the approximate Jacobian, $G$. Note the true Jacobian matrix is
\begin{align}
F'(\bm{\hat{k}}) = 
\begin{bmatrix} M \\ 
& \ddots \\ 
& & M
\end{bmatrix}
- 
\delta t
\begin{bmatrix} {\cal L}_1 \\ 
& \ddots \\ 
& & {\cal L}_s
\end{bmatrix}
(A_0 \otimes I)
\end{align}
where
\begin{align}
{\cal L}_i \coloneqq {\cal N}' \left(\bm{u}_n + \delta t \sum \limits_{j=1}^s a_{ij} \bm{\hat{k}}_j, t_n + c_i \delta t \right).
\end{align}

In the formulation, we factor out a part of the Schur decomposition of $A_0 = Q_0 R_0 Q_0^\top$, and, so the linear systems we actually solve during a Newton-like step take the form
\begin{align}
\big[ R_0 \otimes M - (Q_0^\top \otimes  I) L (Q_0 \otimes I) \big] 
(R_0^{-1} Q_0^\top \otimes I) \bm{d} = - (Q_0^\top \otimes I) F( \bm{k}^0).
\end{align}
where $L \approx {\rm diag}({\cal L}_1, \ldots, {\cal L}_s)$.

During each Newton iteration, we first solve the system
\begin{align}
\big[ R_0 \otimes M - (Q_0^\top \otimes  I) L (Q_0 \otimes I) \big]  \bm{z} = \bm{y}
\end{align}
via backward substitution, where $ \bm{y} = - (Q_0^\top \otimes I) F( \bm{k}^0)$. Then we solve for $\bm{d} = (Q_0 R_0 \otimes I) \bm{z}$ via simple multiplication.




\tcp{An alternative approach... (actually, looking at Will's paper, this is how he does it.)}
Say we make the following simple change of variables:
\begin{align}
\bm{w} = (A_0 \otimes I) \bm{k} 
\quad 
\Longleftrightarrow 
\quad
\bm{k} = (A_0^{-1} \otimes I) \bm{w}
\end{align}
and we first solve the nonlinear problem for $\bm{w}$, then solve a linear problem to obtain $\bm{k}$ from $\bm{w}$ (well, we'd actually obtain $(\bm{b}_0^\top \otimes I) \bm{k} = (\bm{b}_0^\top A_0^{-1} \otimes I) \bm{w} = (\bm{d}_0^\top \otimes I) \bm{w}$ since this is what we really need). Now the nonlinear system becomes
\begin{align}
F(\bm{w}) = ( A_0^{-1} \otimes M ) \bm{w} - 
\begin{bmatrix}
{\cal N}(\bm{u}_{n} + \delta t \bm{w}_1, t_n + c_1 \delta t)\\
\vdots\\
{\cal N}(\bm{u}_{n} + \delta t \bm{w}_s, t_n + c_s \delta t)\\
\end{bmatrix}
= 
0.
\end{align}
What's interesting here is that the components of $\bm{w}$ are now only linearly coupled to each other (via the $A_0^{-1} \otimes M$ term), where with the previous formulation the components of $\bm{k}$ were nonlinearly coupled (every equation involves the evaluation the Jacobian of ${\cal N}$ with $(\bm{k}_i)_{i =1}^s$). The Jacobian is
\begin{align}
F'(\bm{w}^0) = A_0^{-1} \otimes M - 
\delta t
\begin{bmatrix} {\cal L}_1 \\ 
& \ddots \\ 
& & {\cal L}_s
\end{bmatrix}
\end{align}
where
\begin{align}
{\cal L}_i \coloneqq {\cal N}' \left(\bm{u}_n + \delta t \bm{w}_i^0, t_n + c_i \delta t \right).
\end{align}
And now the linear systems we solve during the Newton-like solve take the form
\begin{align}
\big[ R_0 \otimes M - (Q_0^\top \otimes  I) L  (Q_0 \otimes I) \big] 
(Q_0^\top \otimes I) \bm{d} = - (Q_0^\top \otimes I) F( \bm{w}^0).
\end{align}
At first I thought this also saved some work since we didn't need to take linear combinations of the unknowns to evaluate $F$ and its Jacobian, but this is not true: The first term requires we take the same linear combinations with $\bm{w}$.

\end{document}



ADJUGATE FORMS, b^T * A0^{-1} * Adj(Ms)
Let M = A0^{-1}, with entries {m_ij}, b = b[b1,...,bs], and xx = spatial operator L

Stiffly accurate RK (b0^TA0^{-1} = [0,...,0,1])
-----------------------------------------------
s = 2
  -m21,
  m11 - xx

s = 3
  -m22 m31 + m21 m32 + m31 xx, 
  m12 m31 - m11 m32 + m32 xx,
  -m12 m21 + m11 m22 - m11 xx - m22 xx + xx^2

s = 4
  m23 m32 m41 - m22 m33 m41 - m23 m31 m42 + m21 m33 m42 + m22 m31 m43 - 
	 m21 m32 m43 + m22 m41 xx + m33 m41 xx - m21 m42 xx - m31 m43 xx - 
	 m41 xx^2,
  -m13 m32 m41 + m12 m33 m41 + m13 m31 m42 - m11 m33 m42 - 
	 m12 m31 m43 + m11 m32 m43 - m12 m41 xx + m11 m42 xx + m33 m42 xx - 
	 m32 m43 xx - m42 xx^2, 
  m13 m22 m41 - m12 m23 m41 - m13 m21 m42 + m11 m23 m42 + m12 m21 m43 - 
	 m11 m22 m43 - m13 m41 xx - m23 m42 xx + m11 m43 xx + m22 m43 xx - 
	 m43 xx^2,
  -m13 m22 m31 + m12 m23 m31 + m13 m21 m32 - m11 m23 m32 - 
	 m12 m21 m33 + m11 m22 m33 + m12 m21 xx - m11 m22 xx + m13 m31 xx + 
	 m23 m32 xx - m11 m33 xx - m22 m33 xx + m11 xx^2 + m22 xx^2 + 
	 m33 xx^2 - xx^3

s = 5
  m24 m33 m42 m51 - m23 m34 m42 m51 - m24 m32 m43 m51 + 
	  m22 m34 m43 m51 + m23 m32 m44 m51 - m22 m33 m44 m51 - 
	  m24 m33 m41 m52 + m23 m34 m41 m52 + m24 m31 m43 m52 - 
	  m21 m34 m43 m52 - m23 m31 m44 m52 + m21 m33 m44 m52 + 
	  m24 m32 m41 m53 - m22 m34 m41 m53 - m24 m31 m42 m53 + 
	  m21 m34 m42 m53 + m22 m31 m44 m53 - m21 m32 m44 m53 - 
	  m23 m32 m41 m54 + m22 m33 m41 m54 + m23 m31 m42 m54 - 
	  m21 m33 m42 m54 - m22 m31 m43 m54 + m21 m32 m43 m54 - 
	  m23 m32 m51 xx + m22 m33 m51 xx - m24 m42 m51 xx - m34 m43 m51 xx + 
	  m22 m44 m51 xx + m33 m44 m51 xx + m23 m31 m52 xx - m21 m33 m52 xx + 
	  m24 m41 m52 xx - m21 m44 m52 xx - m22 m31 m53 xx + m21 m32 m53 xx + 
	  m34 m41 m53 xx - m31 m44 m53 xx - m22 m41 m54 xx - m33 m41 m54 xx + 
	  m21 m42 m54 xx + m31 m43 m54 xx - m22 m51 xx^2 - m33 m51 xx^2 - 
	  m44 m51 xx^2 + m21 m52 xx^2 + m31 m53 xx^2 + m41 m54 xx^2 + 
	  m51 xx^3,
  -m14 m33 m42 m51 + m13 m34 m42 m51 + m14 m32 m43 m51 - 
	  m12 m34 m43 m51 - m13 m32 m44 m51 + m12 m33 m44 m51 + 
	  m14 m33 m41 m52 - m13 m34 m41 m52 - m14 m31 m43 m52 + 
	  m11 m34 m43 m52 + m13 m31 m44 m52 - m11 m33 m44 m52 - 
	  m14 m32 m41 m53 + m12 m34 m41 m53 + m14 m31 m42 m53 - 
	  m11 m34 m42 m53 - m12 m31 m44 m53 + m11 m32 m44 m53 + 
	  m13 m32 m41 m54 - m12 m33 m41 m54 - m13 m31 m42 m54 + 
	  m11 m33 m42 m54 + m12 m31 m43 m54 - m11 m32 m43 m54 + 
	  m13 m32 m51 xx - m12 m33 m51 xx + m14 m42 m51 xx - m12 m44 m51 xx - 
	  m13 m31 m52 xx + m11 m33 m52 xx - m14 m41 m52 xx - m34 m43 m52 xx + 
	  m11 m44 m52 xx + m33 m44 m52 xx + m12 m31 m53 xx - m11 m32 m53 xx + 
	  m34 m42 m53 xx - m32 m44 m53 xx + m12 m41 m54 xx - m11 m42 m54 xx - 
	  m33 m42 m54 xx + m32 m43 m54 xx + m12 m51 xx^2 - m11 m52 xx^2 - 
	  m33 m52 xx^2 - m44 m52 xx^2 + m32 m53 xx^2 + m42 m54 xx^2 + 
	  m52 xx^3, 
  m14 m23 m42 m51 - m13 m24 m42 m51 - m14 m22 m43 m51 + 
	  m12 m24 m43 m51 + m13 m22 m44 m51 - m12 m23 m44 m51 - 
	  m14 m23 m41 m52 + m13 m24 m41 m52 + m14 m21 m43 m52 - 
	  m11 m24 m43 m52 - m13 m21 m44 m52 + m11 m23 m44 m52 + 
	  m14 m22 m41 m53 - m12 m24 m41 m53 - m14 m21 m42 m53 + 
	  m11 m24 m42 m53 + m12 m21 m44 m53 - m11 m22 m44 m53 - 
	  m13 m22 m41 m54 + m12 m23 m41 m54 + m13 m21 m42 m54 - 
	  m11 m23 m42 m54 - m12 m21 m43 m54 + m11 m22 m43 m54 - 
	  m13 m22 m51 xx + m12 m23 m51 xx + m14 m43 m51 xx - m13 m44 m51 xx + 
	  m13 m21 m52 xx - m11 m23 m52 xx + m24 m43 m52 xx - m23 m44 m52 xx - 
	  m12 m21 m53 xx + m11 m22 m53 xx - m14 m41 m53 xx - m24 m42 m53 xx + 
	  m11 m44 m53 xx + m22 m44 m53 xx + m13 m41 m54 xx + m23 m42 m54 xx - 
	  m11 m43 m54 xx - m22 m43 m54 xx + m13 m51 xx^2 + m23 m52 xx^2 - 
	  m11 m53 xx^2 - m22 m53 xx^2 - m44 m53 xx^2 + m43 m54 xx^2 + 
	  m53 xx^3,
  -m14 m23 m32 m51 + m13 m24 m32 m51 + m14 m22 m33 m51 - 
	  m12 m24 m33 m51 - m13 m22 m34 m51 + m12 m23 m34 m51 + 
	  m14 m23 m31 m52 - m13 m24 m31 m52 - m14 m21 m33 m52 + 
	  m11 m24 m33 m52 + m13 m21 m34 m52 - m11 m23 m34 m52 - 
	  m14 m22 m31 m53 + m12 m24 m31 m53 + m14 m21 m32 m53 - 
	  m11 m24 m32 m53 - m12 m21 m34 m53 + m11 m22 m34 m53 + 
	  m13 m22 m31 m54 - m12 m23 m31 m54 - m13 m21 m32 m54 + 
	  m11 m23 m32 m54 + m12 m21 m33 m54 - m11 m22 m33 m54 - 
	  m14 m22 m51 xx + m12 m24 m51 xx - m14 m33 m51 xx + m13 m34 m51 xx + 
	  m14 m21 m52 xx - m11 m24 m52 xx - m24 m33 m52 xx + m23 m34 m52 xx + 
	  m14 m31 m53 xx + m24 m32 m53 xx - m11 m34 m53 xx - m22 m34 m53 xx - 
	  m12 m21 m54 xx + m11 m22 m54 xx - m13 m31 m54 xx - m23 m32 m54 xx + 
	  m11 m33 m54 xx + m22 m33 m54 xx + m14 m51 xx^2 + m24 m52 xx^2 + 
	  m34 m53 xx^2 - m11 m54 xx^2 - m22 m54 xx^2 - m33 m54 xx^2 + 
	  m54 xx^3, 
  m14 m23 m32 m41 - m13 m24 m32 m41 - m14 m22 m33 m41 + 
	  m12 m24 m33 m41 + m13 m22 m34 m41 - m12 m23 m34 m41 - 
	  m14 m23 m31 m42 + m13 m24 m31 m42 + m14 m21 m33 m42 - 
	  m11 m24 m33 m42 - m13 m21 m34 m42 + m11 m23 m34 m42 + 
	  m14 m22 m31 m43 - m12 m24 m31 m43 - m14 m21 m32 m43 + 
	  m11 m24 m32 m43 + m12 m21 m34 m43 - m11 m22 m34 m43 - 
	  m13 m22 m31 m44 + m12 m23 m31 m44 + m13 m21 m32 m44 - 
	  m11 m23 m32 m44 - m12 m21 m33 m44 + m11 m22 m33 m44 + 
	  m13 m22 m31 xx - m12 m23 m31 xx - m13 m21 m32 xx + m11 m23 m32 xx + 
	  m12 m21 m33 xx - m11 m22 m33 xx + m14 m22 m41 xx - m12 m24 m41 xx + 
	  m14 m33 m41 xx - m13 m34 m41 xx - m14 m21 m42 xx + m11 m24 m42 xx + 
	  m24 m33 m42 xx - m23 m34 m42 xx - m14 m31 m43 xx - m24 m32 m43 xx + 
	  m11 m34 m43 xx + m22 m34 m43 xx + m12 m21 m44 xx - m11 m22 m44 xx + 
	  m13 m31 m44 xx + m23 m32 m44 xx - m11 m33 m44 xx - m22 m33 m44 xx - 
	  m12 m21 xx^2 + m11 m22 xx^2 - m13 m31 xx^2 - m23 m32 xx^2 + 
	  m11 m33 xx^2 + m22 m33 xx^2 - m14 m41 xx^2 - m24 m42 xx^2 - 
	  m34 m43 xx^2 + m11 m44 xx^2 + m22 m44 xx^2 + m33 m44 xx^2 - 
	  m11 xx^3 - m22 xx^3 - m33 xx^3 - m44 xx^3 + xx^4

Full implicit RK
----------------
s = 2
  -b1 m12 m21 + b1 m11 m22 - b1 m11 xx - b2 m21 xx,
  -b2 m12 m21 + b2 m11 m22 - b1 m12 xx - b2 m22 xx

s = 3
  -b1 m13 m22 m31 + b1 m12 m23 m31 + b1 m13 m21 m32 - b1 m11 m23 m32 -
     b1 m12 m21 m33 + b1 m11 m22 m33 + b1 m12 m21 xx - b1 m11 m22 xx + 
     b1 m13 m31 xx - b3 m22 m31 xx + b2 m23 m31 xx + b3 m21 m32 xx - 
     b1 m11 m33 xx - b2 m21 m33 xx + b1 m11 xx^2 + b2 m21 xx^2 + 
     b3 m31 xx^2,
  -b2 m13 m22 m31 + b2 m12 m23 m31 + b2 m13 m21 m32 - 
     b2 m11 m23 m32 - b2 m12 m21 m33 + b2 m11 m22 m33 + b2 m12 m21 xx - 
     b2 m11 m22 xx + b3 m12 m31 xx - b3 m11 m32 xx + b1 m13 m32 xx + 
     b2 m23 m32 xx - b1 m12 m33 xx - b2 m22 m33 xx + b1 m12 xx^2 + 
     b2 m22 xx^2 + b3 m32 xx^2,
  -b3 m13 m22 m31 + b3 m12 m23 m31 + 
     b3 m13 m21 m32 - b3 m11 m23 m32 - b3 m12 m21 m33 + b3 m11 m22 m33 +
     b2 m13 m21 xx - b1 m13 m22 xx - b2 m11 m23 xx + b1 m12 m23 xx + 
     b3 m13 m31 xx + b3 m23 m32 xx - b3 m11 m33 xx - b3 m22 m33 xx + 
     b1 m13 xx^2 + b2 m23 xx^2 + b3 m33 xx^2

s = 4
  b1 m14 m23 m32 m41 - b1 m13 m24 m32 m41 - b1 m14 m22 m33 m41 + 
	  b1 m12 m24 m33 m41 + b1 m13 m22 m34 m41 - b1 m12 m23 m34 m41 - 
	  b1 m14 m23 m31 m42 + b1 m13 m24 m31 m42 + b1 m14 m21 m33 m42 - 
	  b1 m11 m24 m33 m42 - b1 m13 m21 m34 m42 + b1 m11 m23 m34 m42 + 
	  b1 m14 m22 m31 m43 - b1 m12 m24 m31 m43 - b1 m14 m21 m32 m43 + 
	  b1 m11 m24 m32 m43 + b1 m12 m21 m34 m43 - b1 m11 m22 m34 m43 - 
	  b1 m13 m22 m31 m44 + b1 m12 m23 m31 m44 + b1 m13 m21 m32 m44 - 
	  b1 m11 m23 m32 m44 - b1 m12 m21 m33 m44 + b1 m11 m22 m33 m44 + 
	  b1 m13 m22 m31 xx - b1 m12 m23 m31 xx - b1 m13 m21 m32 xx + 
	  b1 m11 m23 m32 xx + b1 m12 m21 m33 xx - b1 m11 m22 m33 xx + 
	  b1 m14 m22 m41 xx - b1 m12 m24 m41 xx + b4 m23 m32 m41 xx - 
	  b3 m24 m32 m41 xx + b1 m14 m33 m41 xx - b4 m22 m33 m41 xx + 
	  b2 m24 m33 m41 xx - b1 m13 m34 m41 xx + b3 m22 m34 m41 xx - 
	  b2 m23 m34 m41 xx - b1 m14 m21 m42 xx + b1 m11 m24 m42 xx - 
	  b4 m23 m31 m42 xx + b3 m24 m31 m42 xx + b4 m21 m33 m42 xx - 
	  b3 m21 m34 m42 xx - b1 m14 m31 m43 xx + b4 m22 m31 m43 xx - 
	  b2 m24 m31 m43 xx - b4 m21 m32 m43 xx + b1 m11 m34 m43 xx + 
	  b2 m21 m34 m43 xx + b1 m12 m21 m44 xx - b1 m11 m22 m44 xx + 
	  b1 m13 m31 m44 xx - b3 m22 m31 m44 xx + b2 m23 m31 m44 xx + 
	  b3 m21 m32 m44 xx - b1 m11 m33 m44 xx - b2 m21 m33 m44 xx - 
	  b1 m12 m21 xx^2 + b1 m11 m22 xx^2 - b1 m13 m31 xx^2 + 
	  b3 m22 m31 xx^2 - b2 m23 m31 xx^2 - b3 m21 m32 xx^2 + 
	  b1 m11 m33 xx^2 + b2 m21 m33 xx^2 - b1 m14 m41 xx^2 + 
	  b4 m22 m41 xx^2 - b2 m24 m41 xx^2 + b4 m33 m41 xx^2 - 
	  b3 m34 m41 xx^2 - b4 m21 m42 xx^2 - b4 m31 m43 xx^2 + 
	  b1 m11 m44 xx^2 + b2 m21 m44 xx^2 + b3 m31 m44 xx^2 - b1 m11 xx^3 - 
	  b2 m21 xx^3 - b3 m31 xx^3 - b4 m41 xx^3, 
   b2 m14 m23 m32 m41 - b2 m13 m24 m32 m41 - b2 m14 m22 m33 m41 + 
	  b2 m12 m24 m33 m41 + b2 m13 m22 m34 m41 - b2 m12 m23 m34 m41 - 
	  b2 m14 m23 m31 m42 + b2 m13 m24 m31 m42 + b2 m14 m21 m33 m42 - 
	  b2 m11 m24 m33 m42 - b2 m13 m21 m34 m42 + b2 m11 m23 m34 m42 + 
	  b2 m14 m22 m31 m43 - b2 m12 m24 m31 m43 - b2 m14 m21 m32 m43 + 
	  b2 m11 m24 m32 m43 + b2 m12 m21 m34 m43 - b2 m11 m22 m34 m43 - 
	  b2 m13 m22 m31 m44 + b2 m12 m23 m31 m44 + b2 m13 m21 m32 m44 - 
	  b2 m11 m23 m32 m44 - b2 m12 m21 m33 m44 + b2 m11 m22 m33 m44 + 
	  b2 m13 m22 m31 xx - b2 m12 m23 m31 xx - b2 m13 m21 m32 xx + 
	  b2 m11 m23 m32 xx + b2 m12 m21 m33 xx - b2 m11 m22 m33 xx + 
	  b2 m14 m22 m41 xx - b2 m12 m24 m41 xx - b4 m13 m32 m41 xx + 
	  b3 m14 m32 m41 xx + b4 m12 m33 m41 xx - b3 m12 m34 m41 xx - 
	  b2 m14 m21 m42 xx + b2 m11 m24 m42 xx + b4 m13 m31 m42 xx - 
	  b3 m14 m31 m42 xx - b4 m11 m33 m42 xx + b1 m14 m33 m42 xx + 
	  b2 m24 m33 m42 xx + b3 m11 m34 m42 xx - b1 m13 m34 m42 xx - 
	  b2 m23 m34 m42 xx - b4 m12 m31 m43 xx + b4 m11 m32 m43 xx - 
	  b1 m14 m32 m43 xx - b2 m24 m32 m43 xx + b1 m12 m34 m43 xx + 
	  b2 m22 m34 m43 xx + b2 m12 m21 m44 xx - b2 m11 m22 m44 xx + 
	  b3 m12 m31 m44 xx - b3 m11 m32 m44 xx + b1 m13 m32 m44 xx + 
	  b2 m23 m32 m44 xx - b1 m12 m33 m44 xx - b2 m22 m33 m44 xx - 
	  b2 m12 m21 xx^2 + b2 m11 m22 xx^2 - b3 m12 m31 xx^2 + 
	  b3 m11 m32 xx^2 - b1 m13 m32 xx^2 - b2 m23 m32 xx^2 + 
	  b1 m12 m33 xx^2 + b2 m22 m33 xx^2 - b4 m12 m41 xx^2 + 
	  b4 m11 m42 xx^2 - b1 m14 m42 xx^2 - b2 m24 m42 xx^2 + 
	  b4 m33 m42 xx^2 - b3 m34 m42 xx^2 - b4 m32 m43 xx^2 + 
	  b1 m12 m44 xx^2 + b2 m22 m44 xx^2 + b3 m32 m44 xx^2 - b1 m12 xx^3 - 
	  b2 m22 xx^3 - b3 m32 xx^3 - b4 m42 xx^3, 
   b3 m14 m23 m32 m41 - b3 m13 m24 m32 m41 - b3 m14 m22 m33 m41 + 
	  b3 m12 m24 m33 m41 + b3 m13 m22 m34 m41 - b3 m12 m23 m34 m41 - 
	  b3 m14 m23 m31 m42 + b3 m13 m24 m31 m42 + b3 m14 m21 m33 m42 - 
	  b3 m11 m24 m33 m42 - b3 m13 m21 m34 m42 + b3 m11 m23 m34 m42 + 
	  b3 m14 m22 m31 m43 - b3 m12 m24 m31 m43 - b3 m14 m21 m32 m43 + 
	  b3 m11 m24 m32 m43 + b3 m12 m21 m34 m43 - b3 m11 m22 m34 m43 - 
	  b3 m13 m22 m31 m44 + b3 m12 m23 m31 m44 + b3 m13 m21 m32 m44 - 
	  b3 m11 m23 m32 m44 - b3 m12 m21 m33 m44 + b3 m11 m22 m33 m44 + 
	  b3 m13 m22 m31 xx - b3 m12 m23 m31 xx - b3 m13 m21 m32 xx + 
	  b3 m11 m23 m32 xx + b3 m12 m21 m33 xx - b3 m11 m22 m33 xx + 
	  b4 m13 m22 m41 xx - b4 m12 m23 m41 xx + b2 m14 m23 m41 xx - 
	  b2 m13 m24 m41 xx + b3 m14 m33 m41 xx - b3 m13 m34 m41 xx - 
	  b4 m13 m21 m42 xx + b4 m11 m23 m42 xx - b1 m14 m23 m42 xx + 
	  b1 m13 m24 m42 xx + b3 m24 m33 m42 xx - b3 m23 m34 m42 xx + 
	  b4 m12 m21 m43 xx - b2 m14 m21 m43 xx - b4 m11 m22 m43 xx + 
	  b1 m14 m22 m43 xx + b2 m11 m24 m43 xx - b1 m12 m24 m43 xx - 
	  b3 m14 m31 m43 xx - b3 m24 m32 m43 xx + b3 m11 m34 m43 xx + 
	  b3 m22 m34 m43 xx + b2 m13 m21 m44 xx - b1 m13 m22 m44 xx - 
	  b2 m11 m23 m44 xx + b1 m12 m23 m44 xx + b3 m13 m31 m44 xx + 
	  b3 m23 m32 m44 xx - b3 m11 m33 m44 xx - b3 m22 m33 m44 xx - 
	  b2 m13 m21 xx^2 + b1 m13 m22 xx^2 + b2 m11 m23 xx^2 - 
	  b1 m12 m23 xx^2 - b3 m13 m31 xx^2 - b3 m23 m32 xx^2 + 
	  b3 m11 m33 xx^2 + b3 m22 m33 xx^2 - b4 m13 m41 xx^2 - 
	  b4 m23 m42 xx^2 + b4 m11 m43 xx^2 - b1 m14 m43 xx^2 + 
	  b4 m22 m43 xx^2 - b2 m24 m43 xx^2 - b3 m34 m43 xx^2 + 
	  b1 m13 m44 xx^2 + b2 m23 m44 xx^2 + b3 m33 m44 xx^2 - b1 m13 xx^3 - 
	  b2 m23 xx^3 - b3 m33 xx^3 - b4 m43 xx^3, 
   b4 m14 m23 m32 m41 - b4 m13 m24 m32 m41 - b4 m14 m22 m33 m41 + 
	  b4 m12 m24 m33 m41 + b4 m13 m22 m34 m41 - b4 m12 m23 m34 m41 - 
	  b4 m14 m23 m31 m42 + b4 m13 m24 m31 m42 + b4 m14 m21 m33 m42 - 
	  b4 m11 m24 m33 m42 - b4 m13 m21 m34 m42 + b4 m11 m23 m34 m42 + 
	  b4 m14 m22 m31 m43 - b4 m12 m24 m31 m43 - b4 m14 m21 m32 m43 + 
	  b4 m11 m24 m32 m43 + b4 m12 m21 m34 m43 - b4 m11 m22 m34 m43 - 
	  b4 m13 m22 m31 m44 + b4 m12 m23 m31 m44 + b4 m13 m21 m32 m44 - 
	  b4 m11 m23 m32 m44 - b4 m12 m21 m33 m44 + b4 m11 m22 m33 m44 + 
	  b3 m14 m22 m31 xx - b2 m14 m23 m31 xx - b3 m12 m24 m31 xx + 
	  b2 m13 m24 m31 xx - b3 m14 m21 m32 xx + b1 m14 m23 m32 xx + 
	  b3 m11 m24 m32 xx - b1 m13 m24 m32 xx + b2 m14 m21 m33 xx - 
	  b1 m14 m22 m33 xx - b2 m11 m24 m33 xx + b1 m12 m24 m33 xx + 
	  b3 m12 m21 m34 xx - b2 m13 m21 m34 xx - b3 m11 m22 m34 xx + 
	  b1 m13 m22 m34 xx + b2 m11 m23 m34 xx - b1 m12 m23 m34 xx + 
	  b4 m14 m22 m41 xx - b4 m12 m24 m41 xx + b4 m14 m33 m41 xx - 
	  b4 m13 m34 m41 xx - b4 m14 m21 m42 xx + b4 m11 m24 m42 xx + 
	  b4 m24 m33 m42 xx - b4 m23 m34 m42 xx - b4 m14 m31 m43 xx - 
	  b4 m24 m32 m43 xx + b4 m11 m34 m43 xx + b4 m22 m34 m43 xx + 
	  b4 m12 m21 m44 xx - b4 m11 m22 m44 xx + b4 m13 m31 m44 xx + 
	  b4 m23 m32 m44 xx - b4 m11 m33 m44 xx - b4 m22 m33 m44 xx - 
	  b2 m14 m21 xx^2 + b1 m14 m22 xx^2 + b2 m11 m24 xx^2 - 
	  b1 m12 m24 xx^2 - b3 m14 m31 xx^2 - b3 m24 m32 xx^2 + 
	  b1 m14 m33 xx^2 + b2 m24 m33 xx^2 + b3 m11 m34 xx^2 - 
	  b1 m13 m34 xx^2 + b3 m22 m34 xx^2 - b2 m23 m34 xx^2 - 
	  b4 m14 m41 xx^2 - b4 m24 m42 xx^2 - b4 m34 m43 xx^2 + 
	  b4 m11 m44 xx^2 + b4 m22 m44 xx^2 + b4 m33 m44 xx^2 - b1 m14 xx^3 - 
	  b2 m24 xx^3 - b3 m34 xx^3 - b4 m44 xx^3


Computing A0^{-1} = det(A0)^{-1}Adj(A0)
---------------------------------------
From Wikpedia, The adjugate of A is the n×n matrix whose (i,j) entry is the (j,i)
cofactor of A, (-1)^{i+j} * M_ij, where M_ij is the determinant of the principle
minor of A that comes form deleting rows i and j. Moreover, using the Laplace
formula, computing these minors also yields det(A):
	https://en.wikipedia.org/wiki/Determinant#Laplace's_formula_and_the_adjugate_matrix
I think we should make functions that take an MFEM dense matrix and compute determinants
for a given set of rows and columns, e.g., write the following function for 2,3, and 4
sets of rows/columns:

getMinorDet(DenseMatrix A, int row1, int row2, int col1, int col2)
{
	if (row2 >= A.Height() || col2 >0 A.Width()) {
		error
	}
	return A[row1,col1]*A[row2,col2] - A[row1,col2]*A[row2,col1];
}

Using Laplace formula and adjugate/determinant formula for inverse, these would provide
algebraic inverses for RK tableauxs up to s = 5 with minimal code (could go higher, just
need to add more determinants; 5 stages is probably plenty to start).

Det of 3x3:
-m13 m22 m31 + m12 m23 m31 + m13 m21 m32 - m11 m23 m32 - m12 m21 m33 +
  m11 m22 m33

Det of 4x4:
m14 m23 m32 m41 - m13 m24 m32 m41 - m14 m22 m33 m41 + 
 m12 m24 m33 m41 + m13 m22 m34 m41 - m12 m23 m34 m41 - 
 m14 m23 m31 m42 + m13 m24 m31 m42 + m14 m21 m33 m42 - 
 m11 m24 m33 m42 - m13 m21 m34 m42 + m11 m23 m34 m42 + 
 m14 m22 m31 m43 - m12 m24 m31 m43 - m14 m21 m32 m43 + 
 m11 m24 m32 m43 + m12 m21 m34 m43 - m11 m22 m34 m43 - 
 m13 m22 m31 m44 + m12 m23 m31 m44 + m13 m21 m32 m44 - 
 m11 m23 m32 m44 - m12 m21 m33 m44 + m11 m22 m33 m44


